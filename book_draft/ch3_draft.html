<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 2.0.23">
<meta name="author" content="Eyal A. Kazin">
<title>Book Draft</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/*! Asciidoctor default stylesheet | MIT License | https://asciidoctor.org */
/* Uncomment the following line when using as a custom stylesheet */
/* @import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700"; */
html{font-family:sans-serif;-webkit-text-size-adjust:100%}
a{background:none}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
b,strong{font-weight:bold}
abbr{font-size:.9em}
abbr[title]{cursor:help;border-bottom:1px dotted #dddddf;text-decoration:none}
dfn{font-style:italic}
hr{height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
audio,video{display:inline-block}
audio:not([controls]){display:none;height:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type=button],input[type=reset],input[type=submit]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type=checkbox],input[type=radio]{padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,::before,::after{box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;line-height:1;position:relative;cursor:auto;-moz-tab-size:4;-o-tab-size:4;tab-size:4;word-wrap:anywhere;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:0}
p{line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #dddddf;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ul.square{list-style-type:square}
ul.circle ul:not([class]),ul.disc ul:not([class]),ul.square ul:not([class]){list-style:inherit}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:1px solid #dedede;word-wrap:normal}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt{background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
:not(pre).nobreak{word-wrap:normal}
:not(pre).nowrap{white-space:nowrap}
:not(pre).pre-wrap{white-space:pre-wrap}
:not(pre):not([class^=L])>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background:#f7f7f8;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed}
pre{color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;line-height:1.45;text-rendering:optimizeSpeed}
pre code,pre pre{color:inherit;font-size:inherit;line-height:inherit}
pre>code{display:block}
pre.nowrap,pre.nowrap pre{white-space:pre;word-wrap:normal}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background:#f7f7f7;border:1px solid #ccc;border-radius:3px;box-shadow:0 1px 0 rgba(0,0,0,.2),inset 0 0 0 .1em #fff;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin:0 auto;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #dddddf}
#header>h1:only-child{border-bottom:1px solid #dddddf;padding-bottom:8px}
#header .details{border-bottom:1px solid #dddddf;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:flex;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #dddddf;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #e7e7e9;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #dddddf;padding-bottom:8px}
#toc.toc2{margin-top:0!important;background:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #e7e7e9;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #e7e7e9;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border:1px solid #e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:none;background:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:hsla(0,0%,100%,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #e7e7e9}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
details,.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
details{margin-left:1.25rem}
details>summary{cursor:pointer;display:block;position:relative;line-height:1.6;margin-bottom:.625rem;outline:none;-webkit-tap-highlight-color:transparent}
details>summary::-webkit-details-marker{display:none}
details>summary::before{content:"";border:solid transparent;border-left:solid;border-width:.3em 0 .3em .5em;position:absolute;top:.5em;left:-1.25rem;transform:translateX(15%)}
details[open]>summary::before{border:solid transparent;border-top:solid;border-width:.5em .3em 0;transform:translateY(15%)}
details>summary::after{content:"";width:1.25rem;height:1em;position:absolute;top:.3em;left:-1.25rem}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class=paragraph]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6);word-wrap:anywhere}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border:1px solid #e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;border-radius:4px}
.sidebarblock{border:1px solid #dbdbd6;margin-bottom:1.25em;padding:1.25em;background:#f3f3f2;border-radius:4px}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:first-child,.sidebarblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child,.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock>.content>pre{border-radius:4px;overflow-x:auto;padding:1em;font-size:.8125em}
@media screen and (min-width:768px){.literalblock pre,.listingblock>.content>pre{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.listingblock>.content>pre{font-size:1em}}
.literalblock pre,.listingblock>.content>pre:not(.highlight),.listingblock>.content>pre[class=highlight],.listingblock>.content>pre[class^="highlight "]{background:#f7f7f8}
.literalblock.output pre{color:#f7f7f8;background:rgba(0,0,0,.9)}
.listingblock>.content{position:relative}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:inherit;opacity:.5}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:inherit;opacity:.5}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.prettyprint{background:#f7f7f8}
pre.prettyprint .linenums{line-height:1.45;margin-left:2em}
pre.prettyprint li{background:none;list-style-type:inherit;padding-left:0}
pre.prettyprint li code[data-lang]::before{opacity:1}
pre.prettyprint li:not(:first-child) code[data-lang]::before{display:none}
table.linenotable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.linenotable td[class]{color:inherit;vertical-align:top;padding:0;line-height:inherit;white-space:normal}
table.linenotable td.code{padding-left:.75em}
table.linenotable td.linenos,pre.pygments .linenos{border-right:1px solid;opacity:.35;padding-right:.5em;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}
pre.pygments span.linenos{display:inline-block;margin-right:.75em}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock:not(.excerpt)>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.75em;margin-right:.5ex;text-align:right}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans-serif;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract blockquote::before,.quoteblock.excerpt blockquote::before,.quoteblock .quoteblock blockquote::before{display:none}
.quoteblock.abstract blockquote,.quoteblock.abstract p,.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{line-height:1.6;word-spacing:0}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.excerpt>blockquote,.quoteblock .quoteblock{padding:0 0 .25em 1em;border-left:.25em solid #dddddf}
.quoteblock.excerpt,.quoteblock .quoteblock{margin-left:0}
.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{color:inherit;font-size:1.0625rem}
.quoteblock.excerpt .attribution,.quoteblock .quoteblock .attribution{color:inherit;font-size:.85rem;text-align:left;margin-right:0}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content{margin-bottom:1.25em;word-wrap:anywhere}
td.tableblock>.content>:last-child{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>*>tr>*{border-width:1px}
table.grid-cols>*>tr>*{border-width:0 1px}
table.grid-rows>*>tr>*{border-width:1px 0}
table.frame-all{border-width:1px}
table.frame-ends{border-width:1px 0}
table.frame-sides{border-width:0 1px}
table.frame-none>colgroup+*>:first-child>*,table.frame-sides>colgroup+*>:first-child>*{border-top-width:0}
table.frame-none>:last-child>:last-child>*,table.frame-sides>:last-child>:last-child>*{border-bottom-width:0}
table.frame-none>*>tr>:first-child,table.frame-ends>*>tr>:first-child{border-left-width:0}
table.frame-none>*>tr>:last-child,table.frame-ends>*>tr>:last-child{border-right-width:0}
table.stripes-all>*>tr,table.stripes-odd>*>tr:nth-of-type(odd),table.stripes-even>*>tr:nth-of-type(even),table.stripes-hover>*>tr:hover{background:#f8f8f7}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
li>p:empty:only-child::before{content:"";display:inline-block}
ul.checklist>li>p:first-child{margin-left:-1em}
ul.checklist>li>p:first-child>.fa-square-o:first-child,ul.checklist>li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist>li>p:first-child>input[type=checkbox]:first-child{margin-right:.25em}
ul.inline{display:flex;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
td.hdlist2{word-wrap:anywhere}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:4px solid #fff;box-shadow:0 0 0 1px #ddd}
.imageblock.left{margin:.25em .625em 1.25em 0}
.imageblock.right{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active,#footnotes .footnote a:first-of-type:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background:#00fafa}
.black{color:#000}
.black-background{background:#000}
.blue{color:#0000bf}
.blue-background{background:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background:#fa00fa}
.gray{color:#606060}
.gray-background{background:#7d7d7d}
.green{color:#006000}
.green-background{background:#007d00}
.lime{color:#00bf00}
.lime-background{background:#00fa00}
.maroon{color:#600000}
.maroon-background{background:#7d0000}
.navy{color:#000060}
.navy-background{background:#00007d}
.olive{color:#606000}
.olive-background{background:#7d7d00}
.purple{color:#600060}
.purple-background{background:#7d007d}
.red{color:#bf0000}
.red-background{background:#fa0000}
.silver{color:#909090}
.silver-background{background:#bcbcbc}
.teal{color:#006060}
.teal-background{background:#007d7d}
.white{color:#bfbfbf}
.white-background{background:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background:rgba(0,0,0,.8);border-radius:50%;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt,summary{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,td.hdlist1,span.alt,summary{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background:#fffef7;border-color:#e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]{border-bottom:1px dotted}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#header,#content,#footnotes,#footer{max-width:none}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #dddddf!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media amzn-kf8,print{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.3/styles/github.min.css">
</head>
<body class="book toc2 toc-left">
<div id="header">
<h1>Book Draft</h1>
<div class="details">
<span id="author" class="author">Eyal A. Kazin</span><br>
<span id="email" class="email"><a href="mailto:eyalkazin@gmail.com">eyalkazin@gmail.com</a></span><br>
<span id="revnumber">version 1.0,</span>
<span id="revdate">2025-07-23</span>
</div>
<div id="toc" class="toc2">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#_preface">Preface</a></li>
<li><a href="#_part_i_foundations_of_thinking_with_data">Part I: Foundations of Thinking with Data</a>
<ul class="sectlevel1">
<li><a href="#_the_data_science_journey_from_data_to_decisions">1. The Data Science Journey: From Data to Decisions</a>
<ul class="sectlevel2">
<li><a href="#_data_science_as_a_scientific_practice">1.1. Data Science as a Scientific Practice</a></li>
<li><a href="#_the_scientific_workflow">1.2. The Scientific Workflow</a></li>
<li><a href="#_from_correlation_to_causation_from_data_to_action">1.3. From Correlation to Causation, From Data to Action</a></li>
</ul>
</li>
<li><a href="#chapter-combinatorics-probabilities">2. Combinatorics and Probabilities</a>
<ul class="sectlevel2">
<li><a href="#_combinatorics_systematic_counting">2.1. 🧮 Combinatorics: Systematic Counting</a></li>
<li><a href="#_probability_fundamentals">2.2. 🎲 Probability Fundamentals</a>
<ul class="sectlevel3">
<li><a href="#sample-spaces-events">2.2.1. Sample Spaces and Events: All Possible Outcomes and their Subsets</a></li>
</ul>
</li>
<li><a href="#_quantifying_dependence">2.3. 🎲🎲 Quantifying Dependence</a>
<ul class="sectlevel3">
<li><a href="#joint-marginal-probs">2.3.1. Joint and Marginal Probability</a></li>
<li><a href="#section-conditional-probs">2.3.2. Conditional Probabilities</a></li>
<li><a href="#section-chain-rule">2.3.3. The Chain Rule</a></li>
<li><a href="#_bayes_theorem">2.3.4. Bayes' Theorem</a></li>
</ul>
</li>
<li><a href="#_probability_distributions_you_need_to_know">2.4. 🔔 Probability Distributions You Need to Know</a></li>
</ul>
</li>
<li><a href="#_quantifying_information_a_gentle_intro_to_information_theory">3. Quantifying Information - A Gentle Intro to Information Theory</a>
<ul class="sectlevel2">
<li><a href="#section-quantifying-surprise">3.1. 😲 Quantifying Surprise: Self-Information and Bits</a>
<ul class="sectlevel3">
<li><a href="#self-information">3.1.1. Self-Information: The Building Block of Quantifying Information</a></li>
<li><a href="#_units_of_measure_bits_nats">3.1.2. Units of Measure: Bits, Nats</a></li>
<li><a href="#_self_information_in_python">3.1.3. Self-Information in Python</a></li>
<li><a href="#shannon-axioms">3.1.4. Shannon&#8217;s Three Axioms</a></li>
<li><a href="#self-information-intuition">3.1.5. Self-Information Intuition</a></li>
<li><a href="#_relationship_to_log_odds">3.1.6. Relationship to Log-Odds</a></li>
<li><a href="#_self_information_summary">3.1.7. Self-Information Summary</a></li>
</ul>
</li>
<li><a href="#section-entropy">3.2. 🤷 Entropy as the Average Surprise</a>
<ul class="sectlevel3">
<li><a href="#_definition">3.2.1. Definition</a></li>
<li><a href="#_entropy_in_python">3.2.2. Entropy In Python 🐍</a></li>
<li><a href="#_entropy_intuition">3.2.3. Entropy Intuition</a></li>
<li><a href="#_entropy_summary">3.2.4. Entropy Summary</a></li>
</ul>
</li>
<li><a href="#section-quantifying-misalignment">3.3. 🥢 Quantifying Misalignment: Cross-Entropy and KL Divergence</a>
<ul class="sectlevel3">
<li><a href="#_cross_entropy_measuring_the_average_misalignment">3.3.1. Cross-Entropy: Measuring the Average Misalignment</a></li>
<li><a href="#section-kl-divergence">3.3.2. KL Divergence: Measuring the Information Loss</a></li>
<li><a href="#_kl_divergence_interpretation">3.3.3. KL Divergence Interpretation</a></li>
<li><a href="#_cross_entropy_and_kl_divergence_in_python">3.3.4. Cross-Entropy and KL Divergence In Python 🐍</a></li>
<li><a href="#section-kld-not-metric">3.3.5. KL Divergence is a Distance but not a Metric</a></li>
<li><a href="#_quantifying_misalignement_summary">3.3.6. Quantifying Misalignement Summary</a></li>
</ul>
</li>
<li><a href="#section-quantifying-gain">3.4. 🤝 Quantifying Gain</a>
<ul class="sectlevel3">
<li><a href="#_self_information_of_joint_events">3.4.1. Self-Information of Joint Events</a></li>
<li><a href="#shannon-third-axiom">3.4.2. The Third Axiom: Additivity of Independent Events</a></li>
<li><a href="#section-pmi">3.4.3. Point Mutual Information</a></li>
<li><a href="#section-mutual-information">3.4.4. Mutual Information Definition</a></li>
<li><a href="#_mutual_information_from_pmi">Mutual Information from PMI</a></li>
<li><a href="#_mutual_information_from_entropy">Mutual Information from Entropy</a></li>
<li><a href="#_mutual_information_is_not_a_metric">3.4.5. Mutual Information is not a Metric</a></li>
<li><a href="#_summarising_mutual_information">3.4.6. Summarising Mutual Information</a></li>
</ul>
</li>
<li><a href="#section-continuous-info-theory">3.5. 🌊 Information Theory for Continuous Variables</a>
<ul class="sectlevel3">
<li><a href="#section-differential-entropy">3.5.1. The Differential Entropy Problem</a></li>
<li><a href="#section-continuous-kld">3.5.2. KL Divergence for Continuous Data</a></li>
<li><a href="#section-continuous-mi">3.5.3. Mutual Information for Continuous Data</a></li>
</ul>
</li>
<li><a href="#section-info-theory-summary">3.6. Information Theory Summary</a></li>
</ul>
</li>
<li><a href="#_interpreting_associations">4. Interpreting Associations</a></li>
</ul>
</li>
<li><a href="#_part_ii_reasoning_under_uncertainty">Part II Reasoning Under Uncertainty</a>
<ul class="sectlevel1">
<li><a href="#_understanding_and_managing_uncertainty">5. Understanding and Managing Uncertainty</a>
<ul class="sectlevel2">
<li><a href="#uncertainty-types">5.1. Types of Uncertainty</a></li>
</ul>
</li>
<li><a href="#_appendix_a_additional_resources">Appendix A: Appendix A: Additional Resources</a></li>
<li><a href="#_bibliography">Bibliography</a></li>
<li><a href="#_index">Index</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="content">
<div id="preamble">
<div class="sectionbody">
<div class="sidebarblock">
<div class="content">
<div class="paragraph">
<p>Dear reader, this is a draft excerpt of a book on Data Science. It contains the planned structure and preliminary text for Chapter 3, which focuses on Information Theory. The chapter is not yet final and will undergo further editing and refinement. In particular the final version will include additional visuals, Python code examples and links to interactive Jupyter notebooks where you can experiment with the concepts discussed.</p>
</div>
<div class="paragraph">
<p>The other chapters are currently in development. I welcome your feedback and suggestions!</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_preface"><a class="anchor" href="#_preface"></a><a class="link" href="#_preface">Preface</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>The motivation behind this book is to provide a practical, hands-on guide for data scientists, engineers and analysts to navigate the complexities of data-driven decision-making. It aims to bridge the gap between theory  and real-world practice, empowering readers to make informed decisions from their data.</p>
</div>
<div class="paragraph">
<p>In essence, this is the book I wish I&#8217;d had when starting my journey in data science, written as a practical reference to guide others along theirs.</p>
</div>
</div>
</div>
<h1 id="_part_i_foundations_of_thinking_with_data" class="sect0"><a class="anchor" href="#_part_i_foundations_of_thinking_with_data"></a><a class="link" href="#_part_i_foundations_of_thinking_with_data">Part I: Foundations of Thinking with Data</a></h1>
<div class="sect1">
<h2 id="_the_data_science_journey_from_data_to_decisions"><a class="anchor" href="#_the_data_science_journey_from_data_to_decisions"></a><a class="link" href="#_the_data_science_journey_from_data_to_decisions">1. The Data Science Journey: From Data to Decisions</a></h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_data_science_as_a_scientific_practice"><a class="anchor" href="#_data_science_as_a_scientific_practice"></a><a class="link" href="#_data_science_as_a_scientific_practice">1.1. Data Science as a Scientific Practice</a></h3>

</div>
<div class="sect2">
<h3 id="_the_scientific_workflow"><a class="anchor" href="#_the_scientific_workflow"></a><a class="link" href="#_the_scientific_workflow">1.2. The Scientific Workflow</a></h3>

</div>
<div class="sect2">
<h3 id="_from_correlation_to_causation_from_data_to_action"><a class="anchor" href="#_from_correlation_to_causation_from_data_to_action"></a><a class="link" href="#_from_correlation_to_causation_from_data_to_action">1.3. From Correlation to Causation, From Data to Action</a></h3>

</div>
</div>
</div>
<div class="sect1">
<h2 id="chapter-combinatorics-probabilities"><a class="anchor" href="#chapter-combinatorics-probabilities"></a><a class="link" href="#chapter-combinatorics-probabilities">2. Combinatorics and Probabilities</a></h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_combinatorics_systematic_counting"><a class="anchor" href="#_combinatorics_systematic_counting"></a><a class="link" href="#_combinatorics_systematic_counting">2.1. 🧮 Combinatorics: Systematic Counting</a></h3>

</div>
<div class="sect2">
<h3 id="_probability_fundamentals"><a class="anchor" href="#_probability_fundamentals"></a><a class="link" href="#_probability_fundamentals">2.2. 🎲 Probability Fundamentals</a></h3>
<div class="sect3">
<h4 id="sample-spaces-events"><a class="anchor" href="#sample-spaces-events"></a><a class="link" href="#sample-spaces-events">2.2.1. Sample Spaces and Events: All Possible Outcomes and their Subsets</a></h4>

</div>
</div>
<div class="sect2">
<h3 id="_quantifying_dependence"><a class="anchor" href="#_quantifying_dependence"></a><a class="link" href="#_quantifying_dependence">2.3. 🎲🎲 Quantifying Dependence</a></h3>
<div class="sect3">
<h4 id="joint-marginal-probs"><a class="anchor" href="#joint-marginal-probs"></a><a class="link" href="#joint-marginal-probs">2.3.1. Joint and Marginal Probability</a></h4>

</div>
<div class="sect3">
<h4 id="section-conditional-probs"><a class="anchor" href="#section-conditional-probs"></a><a class="link" href="#section-conditional-probs">2.3.2. Conditional Probabilities</a></h4>

</div>
<div class="sect3">
<h4 id="section-chain-rule"><a class="anchor" href="#section-chain-rule"></a><a class="link" href="#section-chain-rule">2.3.3. The Chain Rule</a></h4>

</div>
<div class="sect3">
<h4 id="_bayes_theorem"><a class="anchor" href="#_bayes_theorem"></a><a class="link" href="#_bayes_theorem">2.3.4. Bayes' Theorem</a></h4>

</div>
</div>
<div class="sect2">
<h3 id="_probability_distributions_you_need_to_know"><a class="anchor" href="#_probability_distributions_you_need_to_know"></a><a class="link" href="#_probability_distributions_you_need_to_know">2.4. 🔔 Probability Distributions You Need to Know</a></h3>

</div>
</div>
</div>
<div class="sect1">
<h2 id="_quantifying_information_a_gentle_intro_to_information_theory"><a class="anchor" href="#_quantifying_information_a_gentle_intro_to_information_theory"></a><a class="link" href="#_quantifying_information_a_gentle_intro_to_information_theory">3. Quantifying Information - A Gentle Intro to Information Theory</a></h2>
<div class="sectionbody">
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>[Shannon&#8217;s “A Mathematical Theory of Communication”] - the blueprint for the digital era</p>
</div>
</blockquote>
<div class="attribution">
&#8212; Historian James Gleick
</div>
</div>
<div id="fig-shannon-manuri" class="imageblock text-center">
<div class="content">
<img src="images/chapter-03-info-theory/shannon_with_curta_by_perplexity_munari_style.png" alt="Claude Shannon with a Curta calculator in the style of Munari" width="300">
</div>
<div class="title">Figure 1. Claude Shannon with a Curta calculator in the style of Bruno Munari. Generated by Perplexity.<sup class="footnote">[<a id="_footnoteref_1" class="footnote" href="#_footnotedef_1" title="View footnote.">1</a>]</sup></div>
</div>
<div class="paragraph">
<p>Considered the Magna Carta of the Information Age, Claude Shannon's seminal 1948 paper <em>"A Mathematical Theory of Communication"</em><a href="#shannon1948">[shannon1948]</a> posed a groundbreaking question:</p>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>How can information be quantified?</p>
</div>
</blockquote>
</div>
<div class="paragraph">
<p>This question laid the foundation for information theory, revolutionising technology in ways that are still unfolding today. Shannon&#8217;s insights underpin how we <strong>measure</strong>, <strong>store</strong>, and <strong>transmit</strong> information, contributing to breakthroughs in signal processing, data compression (e.g., Zip files), the Internet, and artificial intelligence. Beyond technology, his work has influenced diverse fields such as neurobiology, statistical physics, and computer science (e.g., cybersecurity, cloud computing, and machine learning).</p>
</div>
<div class="paragraph">
<p>Our focus in this chapter is on the quantification of information, an essential tool for data scientists. Its applications range from strengthening statistical analyses to serving as a go-to decision heuristic in modern machine learning algorithms.</p>
</div>
<div class="paragraph">
<p>In particular we focus on three key measures and their fundamentals: entropy, Kullback-Leibler (KL) divergence and mutual information. These concepts bridge probability theory with real-world applications and serve as practical tools for analysis, data science and optimization in machine learning.</p>
</div>
<div class="paragraph">
<p>Broadly speaking, quantifying information means assessing uncertainty <sup class="footnote">[<a id="_footnoteref_2" class="footnote" href="#_footnotedef_2" title="View footnote.">2</a>]</sup>, which may be phrased as: <em>"How surprising is an outcome?"</em>.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
To build intuition we first focus on analyzing categorical variables with \(c\geq 2\) classes (with \(c=2\) representing the binary case). Extensions to continuous variables will be discussed in <a href="#section-continuous-info-theory">Section 3.5</a>.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>In the following sections, we discuss Shannon&#8217;s axioms to address the question of information quantification to the concrete tools entropy, KL divergence, and mutual information that data scientists rely on today.</p>
</div>
<div class="paragraph">
<p><strong>Concept Map</strong></p>
</div>
<div class="paragraph">
<p>The following is a concept map illustrating the relationships between key concepts in information theory discussed throughout the chapter. Light-colored boxes represent measures defined for individual events (e.g., self-information), while dark-colored boxes represent measures defined over entire distributions (e.g., entropy, KL divergence, mutual information)</p>
</div>
<div id="fig-info-theory-concept-map" class="imageblock text-center">
<div class="content">
<img src="images/chapter-03-info-theory/info_theory_conceptual_map.png" alt="Information Theory Concept Map" width="650">
</div>
<div class="title">Figure 2. Information Theory Concept Map</div>
</div>
<div class="paragraph">
<p>Starting with <a href="#self-information">self-information</a> which quantifies the surprise of a single outcome, we identify two main paths. The first path focuses on single-variable measures, beginning with entropy to capture the average information in a variable, and extending to comparisons between distributions of the same variable through cross-entropy and KL divergence.</p>
</div>
<div class="paragraph">
<p>This will be followed by the second path which considers joint events of two variables (e.g., joint self-information) and extends to measures of joint distributions, culminating in mutual information. Mutual information can be expressed in several equivalent forms, which we will explore in <a href="#section-mutual-information">Section 3.4.4</a>.</p>
</div>
<div class="sect2">
<h3 id="section-quantifying-surprise"><a class="anchor" href="#section-quantifying-surprise"></a><a class="link" href="#section-quantifying-surprise">3.1. 😲 Quantifying Surprise: Self-Information and Bits</a></h3>
<div class="paragraph">
<p>In the context of information theory, we quantify surprise using the concept of <em><strong>self-information</strong></em>. Also known as <em><strong>information content</strong></em> or <strong><em>surprisal</em></strong>, this is a measure of how much information is produced when an <a href="#sample-spaces-events">event</a> occurs, based on its probability. The more surprising an event is, the more information it conveys.
In this section, we will explore the concept of self-information, its units of measure, and the axioms that underpin it.</p>
</div>
<div class="paragraph">
<p>We will also discuss the intuition behind self-information and how it relates to probability.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
Note that self-information is not limited to an event of one parameter, but may quantify the surprise of events from multiple parameters. In the next few sections, however, unless stated otherwise, we will focus on single-parameter events. We will explore self-information of joint events in <a href="#section-quantifying-gain">Section 3.4</a>.
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="self-information"><a class="anchor" href="#self-information"></a><a class="link" href="#self-information">3.1.1. Self-Information: The Building Block of Quantifying Information</a></h4>
<div class="paragraph">
<p>Self-information is a way of quantifying the amount of "surprise" of a specific outcome.</p>
</div>
<div class="paragraph">
<p>Formally self-information denoted here as \(h_x\), quantifies the surprise of an event \(x\) occurring based on its probability, \(p(x)\) :</p>
</div>
<div id="eq-self-information" class="stemblock">
<div class="title">Self-Information Equation</div>
<div class="content">
\[h_x = -\log_2(p(x))\]
</div>
</div>
<div class="paragraph">
<p>Self-information serves as an alternative to probabilities, odds and log-odds, with certain mathematical properties which are advantageous for information theory. We discuss these below when learning about <a href="#shannon-axioms">Shannon&#8217;s axioms</a> behind this choice.</p>
</div>
<div class="paragraph">
<p>One of the key mathematical properties of the logarithm function is that it transforms multiplication into addition, which is useful for combining probabilities. The base of the logarithm can vary, but the most common bases are 2 (binary), \(e\) (natural), and 10 (decimal). The choice of base affects the units of measure, which we will discuss next.</p>
</div>
</div>
<div class="sect3">
<h4 id="_units_of_measure_bits_nats"><a class="anchor" href="#_units_of_measure_bits_nats"></a><a class="link" href="#_units_of_measure_bits_nats">3.1.2. Units of Measure: Bits, Nats</a></h4>
<div class="paragraph">
<p>When using log base 2, the units of measure are called bits. One bit (<strong>b</strong>inary dig<strong>it</strong>) is the amount of information for an event \(x\) that has probability \(p(x)=\frac{1}{2}\). Let&#8217;s plug in to verify: \(h_x(p=0.5)=-\log_2(0.5) = \log_2(2) = 1 \text{ bit}\). This means that if an event has a 50% chance of occurring, it carries 1 bit of information. In other words, if you were to flip a fair coin, the outcome (heads or tails) would provide you with 1 bit of information.</p>
</div>
<div class="paragraph">
<p>When using the natural log the base units are called nats (<strong>nat</strong>ural units of information). One nat corresponds to the information gained from an event occurring with a probability of \(1/e\) where \(e\) is Euler&#8217;s number (\(\approx 2.71828\)). In other words, \(h_x(p=1/e) = -\ln(1/e) = \ln(e) = 1 \text{ nat}\).</p>
</div>
<div class="paragraph">
<p>Key aspects of machine learning, such as popular loss functions, often rely on integrals and derivatives. The natural logarithm is a practical choice in these contexts because it can be derived and integrated without introducing additional constants. This likely explains why the machine learning community frequently uses nats as the unit of information, it simplifies the mathematics by avoiding the need to account for factors like ln(2).</p>
</div>
<div class="paragraph">
<p>That said, throughout this book, unless otherwise stated, I use base 2, mostly because it is the most simple to interpret, but highlight that in machine learning applications nats are the preferred unit of measure. The conversion is straight forward: 1 bit = ln(2) nats ≈ 0.693 nats. Think of it as similar to a monetary current exchange or converting centimeters to inches.</p>
</div>
</div>
<div class="sect3">
<h4 id="_self_information_in_python"><a class="anchor" href="#_self_information_in_python"></a><a class="link" href="#_self_information_in_python">3.1.3. Self-Information in Python</a></h4>
<div class="paragraph">
<p>Self-information can be easily computed in Python using the <code>math</code> module. Here&#8217;s a simple function to calculate self-information given a probability:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">import math

def self_information(probability):
    if probability &lt;= 0 or probability &gt; 1:
        raise ValueError("Invalid probability. Must be between 0 and 1.")
    return -math.log2(probability) #same as: -math.log(p_event)/math.log(2)</code></pre>
</div>
</div>
<div class="paragraph">
<p>This function takes a probability value larger than 0 smaller or equal to 1 and returns the self-information in bits. If the probability is outside this range, it raises a <code>ValueError</code>.</p>
</div>
<div class="paragraph">
<p>Let&#8217;s examine self-information with the simplest case of a fair coin (i.e, 50% chance for success/Heads or failure/Tails).</p>
</div>
<div class="paragraph">
<p>Imagine an area for which at any given day there is a 50:50 chance for sun or rain. We can write the probability of each event be: \(p(🌞)=p(🌧)=0.5\).</p>
</div>
<div class="paragraph">
<p>Let&#8217;s generate a printout function to illustrate the self-information for each event in an array of probabilities <code>p_events</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">import numpy as np

def print_events_self_information(p_events):
    print(f"Given distribution {p_events}")
    for event, p_event in p_events.items():
        if p_event != 0:
            self_information = -np.log2(p_event)
            text_ = f'When `{event}` occurs {self_information:0.2f} bits of information is communicated'
            print(text_)
        else:
            print(f'a `{event}` event cannot happen p=0 ')</code></pre>
</div>
</div>
<div class="paragraph">
<p>Feeding in <code>print_events_self_information({'🌞': 0.5, '🌧': 0.5})</code> we obtain</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">Given distribution {'🌞': 0.5, '🌧': 0.5}
When `🌞` occurs 1.00 bits of information is communicated
When `🌧` occurs 1.00 bits of information is communicated</code></pre>
</div>
</div>
<div class="paragraph">
<p>It&#8217;s illustrative to examine two more examples:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>More sun than rain <code>{'🌞': 0.75, '🌧': 0.25}</code></p>
</li>
<li>
<p>Mostly sunshine <code>{'🌞': 0.99, '🌧': 0.01}</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Plugging these into <code>print_events_self_information</code> we obtain:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">Given distribution {'🌞': 0.75, '🌧': 0.25}
When `🌞` occurs 0.42 bits of information is communicated
When `🌧` occurs 2.00 bits of information is communicated
====================
Given distribution {'🌞': 0.99, '🌧': 0.01}
When `🌞` occurs 0.01 bits of information is communicated
When `🌧` occurs 6.64 bits of information is communicated</code></pre>
</div>
</div>
<div class="paragraph">
<p>This, of course, may be applied to any categorical variable with more than two classes.
For example <code>print_events_self_information([{'🌞': 0.2, '🌧': 0.7, '⛄️': 0.1}])</code> yields</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">Given distribution {'🌞': 0.2, '🌧': 0.7, '⛄️': 0.1}
When `🌞` occurs 2.32 bits of information is communicated
When `🌧` occurs 0.51 bits of information is communicated
When `⛄️` occurs 3.32 bits of information is communicated</code></pre>
</div>
</div>
<div class="paragraph">
<p>Regardless of the number of categories we see similar trends:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>When a highly expected event occurs we do not learn much, the bit count is low.</p>
</li>
<li>
<p>When an unexpected event occurs we learn a lot, the bit count is high.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>These findings follow axioms of self-information, which Shannon formalised in his 1948 paper. We will explore these axioms next.</p>
</div>
</div>
<div class="sect3">
<h4 id="shannon-axioms"><a class="anchor" href="#shannon-axioms"></a><a class="link" href="#shannon-axioms">3.1.4. Shannon&#8217;s Three Axioms</a></h4>
<div class="paragraph">
<p>Shannon chose the log function as a manner to meet three axioms which we will explore in detail:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Axiom 1: An event with probability 100% is not surprising</p>
</li>
<li>
<p>Axiom 2: Less probable events are more surprising and, when they occur, provide more information.</p>
</li>
<li>
<p>Axiom 3: If two independent events are measured separately, the total amount of information is the sum of the self-information of each event.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In the next section we will get an intuition for the first two axioms by visualising
self-information.
Since the the third axiom involves joint events, we will explore it in  <a href="#shannon-third-axiom">Section 3.4.2</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="self-information-intuition"><a class="anchor" href="#self-information-intuition"></a><a class="link" href="#self-information-intuition">3.1.5. Self-Information Intuition</a></h4>
<div class="paragraph">
<p>It&#8217;s informative to explore how an equation behaves with a graph, the following illustrates the <a href="#eq-self-information">Self-Information Equation</a>:</p>
</div>
<div id="fig-self-information" class="imageblock">
<div class="content">
<img src="images/chapter-03-info-theory/self_information.png" alt="The Self Information Graph" width="600" height="400">
</div>
<div class="title">Figure 3. Self Information Graph</div>
</div>
<div class="paragraph">
<p>To emphasise the logarithmic nature of self-information, I&#8217;ve highlighted three points of interest on the graph:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>At \(p=1\) an event is guaranteed, yielding no surprise and hence zero bits of information (zero bits). A useful analogy is a trick coin (where both sides show HEAD).</p>
</li>
<li>
<p>Reducing the probability by a factor of two (\(p=0.5\)​) increases the information to \(h(p=0.5)=1\) bit. This, of course, is the case of a fair coin.</p>
</li>
<li>
<p>Further reducing it by a factor of four results in \(h(p=\frac{1}{8})=3\) bits. For example think of an eight sided die, where the probability of rolling any specific number is \(p=\frac{1}{8}\). This means that if you roll an eight-sided die, the outcome (any one of the eight numbers) would provide you with 3 bits of information.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Here we can identify key properties of self-information, where we focus on the first two axioms:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Zero minimum bound</strong>: Self-information is non-negative, with zero as its lower bound. The \(h(p=1)=0\) feature is a manifestation of the first axiom: An event with probability 100% is not surprising and hence does not yield any information. This is highly practical for many applications. E.g, we shall see this isn the context a loss-function when we discuss classification tasks.</p>
</li>
<li>
<p><strong>Monotonically decreasing</strong>: Self-information decreases monotonically with increasing probability. This is an expression of the second axiom: Less probable events are more surprising and provide more information.</p>
</li>
<li>
<p><strong>No Maximum bound</strong>: At the extreme where \(p\rightarrow 0\), monotonicity leads self-information to grow unbounded \( h(p\rightarrow 0) \rightarrow \infty\), a feature that requires careful consideration in some contexts. However, when averaging self-information, as we will later see in the calculation of entropy (<a href="#eq-entropy">Entropy Equation</a>),probabilities act as weights, effectively limiting the contribution of highly improbable events to the overall average.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The following Python code generates <a href="#fig-self-information">Figure 3</a>, demonstrating the  <a href="#eq-self-information">Self-Information Equation</a>:</p>
</div>
<script src="https://gist.github.com/elzurdo/cb79a97d97d7306f52cde59731665805.js"></script>
</div>
<div class="sect3">
<h4 id="_relationship_to_log_odds"><a class="anchor" href="#_relationship_to_log_odds"></a><a class="link" href="#_relationship_to_log_odds">3.1.6. Relationship to Log-Odds</a></h4>
<div class="paragraph">
<p><strong><em>Log-odds</em></strong>, also known as the <strong><em>logit</em></strong> function, is a transformation commonly used in statistics and machine learning to convert probabilities into a continuous scale that ranges from negative infinity to positive infinity. It is defined as the logarithm of the odds of an event occurring:</p>
</div>
<div id="eq-log-odds" class="stemblock">
<div class="title">Log-Odds Equation</div>
<div class="content">
\[\text{log-odds}(p(x)) \equiv \log\left(\frac{p(x)}{1-p(x)}\right) = h(\neg x) - h(x)\]
</div>
</div>
<div class="paragraph">
<p>The right hand side of the equation shows the relationship between log-odds and self-information. Here, \(h(x)\) is the self-information of the event occurring, and \(h(\neg x)\) is the self-information of the event not occurring (the complement event). This relationship highlights that log-odds can be interpreted as the difference in self-information between an event and its complement.</p>
</div>
</div>
<div class="sect3">
<h4 id="_self_information_summary"><a class="anchor" href="#_self_information_summary"></a><a class="link" href="#_self_information_summary">3.1.7. Self-Information Summary</a></h4>
<div class="paragraph">
<p>We&#8217;ve seen that self-information is a fundamental concept in information theory. It quantifies the surprise of individual events based on their probabilities and expresses this in units of bits or nats, depending on the logarithm base used.</p>
</div>
<div class="paragraph">
<p>Self-information occasionally appears in practice on its own. Some noteworthy uses include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Data Compression</strong>: Self-information sets the theoretical lower bound on the number of bits required to encode an event of a given probability. Compression algorithms such as <em>Huffman</em> or <em>arithmetic coding</em> approach this bound by assigning shorter codes to more probable (low self-information) symbols and longer codes to less probable (high self-information) symbols. As interesting as this topic is, it lies outside the scope of this book, which focuses on interpreting data rather than engineering implementations.</p>
</li>
<li>
<p><strong>Anomaly Detection</strong>: In anomaly detection, self-information can quantify how “surprising” an event is under a reference distribution. Events with very high self-information are rare and may be flagged as anomalies. This is useful in fraud detection, network security, and quality control. We will see this manifest in chapters (TBD).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>While self-information tells us how surprising a single event is, in practice we often want to know the typical or expected surprise across all possible events of a variable.</p>
</div>
<div class="paragraph">
<p>This naturally leads to a broader question: what is the average amount of information associated with all possible outcomes of a variable?</p>
</div>
<div class="paragraph">
<p>That will be the focus of the rest of this chapter, where we will explore entropy in <a href="#section-entropy">Section 3.2</a> followed by
its derivatives such as KL divergence in <a href="#section-kl-divergence">Section 3.3.2</a> and mutual information in <a href="#section-mutual-information">Section 3.4.4</a>.
These three metrics extend the idea of surprise from individual events to entire distributions and form the cornerstones of information theory, with numerous applications in data science and machine learning.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="section-entropy"><a class="anchor" href="#section-entropy"></a><a class="link" href="#section-entropy">3.2. 🤷 Entropy as the Average Surprise</a></h3>
<div class="paragraph">
<p>Entropy is a fundamental concept in information theory, serving as a measure of randomness or uncertainty in a probability distribution. It quantifies the average amount of information produced by a stochastic source of data. In this section, we will explore the definition of entropy, its mathematical formulation, and its significance in data science and machine learning.</p>
</div>
<div class="sect3">
<h4 id="_definition"><a class="anchor" href="#_definition"></a><a class="link" href="#_definition">3.2.1. Definition</a></h4>
<div class="paragraph">
<p>So far we&#8217;ve discussed the amount of information in bits per event. This raises the question: what is the <em>average amount of information</em> we may learn from the distribution of a variable?</p>
</div>
<div class="paragraph">
<p>This is called entropy and may be considered as the uncertainty or average “element of surprise” ¯\\_(ツ)_//¯. This means how much information may be learnt, on average, when the variable value is determined. In other words, entropy is the average self-information.</p>
</div>
<div class="paragraph">
<p>Formally: given a categorical random variable \(X\), with \(c\) possible outcomes \(x_i\) , \(i\in\{1…c\}\), each with probability \(p_{X}(x_{i})\), the entropy \(H_{X}\) is:</p>
</div>
<div id="eq-entropy" class="stemblock">
<div class="title">Entropy Equation</div>
<div class="content">
\[H_{X} = \sum_{i=1}^{c} p_{X}(x_{i}) h_{X}(x_{i}) = -\sum_{i=1}^{c} p_{X}(x_{i}) \log_{2} p_{X}(x_{i})\]
</div>
</div>
<div class="paragraph">
<p>The intuition here is that</p>
</div>
<div class="ulist">
<ul>
<li>
<p>\(h_{X}(x)=-\log_{2}(p_{X}(x))\): the <a href="#self-information">self-information</a> for each event \(x\).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This is multiplied by</p>
</div>
<div class="ulist">
<ul>
<li>
<p>\(p_{X}(x)\): the weight to the expectancy of its occurrence (i.e, think of the left \(p_{X}(x)\) as a weight \(w_{X}(x)\) of event \(x\)).</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_entropy_in_python"><a class="anchor" href="#_entropy_in_python"></a><a class="link" href="#_entropy_in_python">3.2.2. Entropy In Python 🐍</a></h4>
<div class="paragraph">
<p>A naïve pythonic calculation would look something like this</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">import numpy as np
pxs = [0.5, 0.5] # fair coin distribution: 50% tails, 50% heads
np.sum([-px * np.log2(px) for px in pxs])

# yields 1 bit</code></pre>
</div>
</div>
<div class="paragraph">
<p>However, it is more pragmatic to use the scipy module:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">from scipy.stats import entropy
entropy(pxs, base=2) # note the base keyword!

# yields 1 bit</code></pre>
</div>
</div>
<div class="paragraph">
<p>This function is preferable because it addresses practical issues that the naïve script above it doesn&#8217;t:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Handling of zero values. This is crucial. Try plugging in <code>pxs=[1., 0.]</code> in the numpy version and you will obtain nan due to a <code>RuntimeWarning</code>. This is because 0 is not a valid input to log functions. If you plug it into the scipy version you will obtain the correct 0 bits.</p>
</li>
<li>
<p>Normalised counts. You can feed counts instead of frequencies, e.g, try plugging in <code>pxs=[14, 14]</code> you will still get 1 bit.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_entropy_intuition"><a class="anchor" href="#_entropy_intuition"></a><a class="link" href="#_entropy_intuition">3.2.3. Entropy Intuition</a></h4>
<div class="paragraph">
<p>Perhaps one of the most important figures that should be ingrained in every data scientist
is the entropy of Bernoulli trials (\(c=2\)), which is the entropy of a binary random variable:</p>
</div>
<div id="fig-bernoulli-entropy" class="imageblock text-center">
<div class="content">
<img src="images/chapter-03-info-theory/bernoulli_entropy.png" alt="The Bernoulli Entropy Curve" width="600" height="400">
</div>
<div class="title">Figure 4. The Bernoulli Entropy Curve</div>
</div>
<div class="paragraph">
<p>There are many learnings from this graph which may be extrapolated to any categorical variable with \(c\) dimensions.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Max Uncertainty: The max entropy (uncertainty) happens in the case of a fair coin \(p=\frac{1}{2} \rightarrow H=1\) bit. For a die with  \(c\) sides this would manifest if all classes have probability \(p=1/c\).</p>
</li>
<li>
<p>Zero valued uncertainty points: We see that in the cases of \(p=0\) and \(p=1\) there is no uncertainty. In other words \(H=0\) bits means full certainty of the outcome of the variable. For the \(c\) sided die this would manifest if all classes have probability \(p=0\), except for one with \(p=1\). Remember that self-information is not defined at \(p=0\)? The \(p=0\) weight cancled out this effect.</p>
</li>
<li>
<p>Symmetry: By definition \(H(x)\) is symmetric around \(p=1/c\). The graph above demonstrates this in the binary case around \(p=\frac{1}{2}\).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To generate <a href="#fig-bernoulli-entropy">Figure 4</a> in Python, you can use the following code:</p>
</div>
<script src="https://gist.github.com/elzurdo/45284e01ffccf828e5f26e8c82e267e0.js"></script>
</div>
<div class="sect3">
<h4 id="_entropy_summary"><a class="anchor" href="#_entropy_summary"></a><a class="link" href="#_entropy_summary">3.2.4. Entropy Summary</a></h4>
<div class="paragraph">
<p>Entropy is a versatile concept with numerous applications in the sciences<sup class="footnote">[<a id="_footnoteref_3" class="footnote" href="#_footnotedef_3" title="View footnote.">3</a>]</sup>, data science and machine learning.</p>
</div>
<div class="paragraph">
<p>Throughout this book you will encounter entropy, and its derivatives (e.g, KL divergence and mutual information), in various contexts. At its core, entropy itself is especially useful in algorithms that require measuring purity or impurity.</p>
</div>
<div class="paragraph">
<p>One example is in decision trees, where entropy may be used to guide how to split data by assessing the homogeneity of leaves. For a hands-on python tutorial of this usecase see the Decision Tree Splitting notebook (link TBD).</p>
</div>
<div class="paragraph">
<p>Another is entropy as a diversity index when quantifying the variety of a dataset<sup class="footnote">[<a id="_footnoteref_4" class="footnote" href="#_footnotedef_4" title="View footnote.">4</a>]</sup>. For a hands-on python tutorial of this usecase see the Entropy as Diversity Application: DNA Library Verification 🧬 notebook (link TBD).</p>
</div>
<div class="paragraph">
<p>As insightful as entropy is, it applies when the true (or assumed true) probability distribution is known. But what if we want to compare a predicted distribution against a ground truth?</p>
</div>
<div class="paragraph">
<p>In those cases, we turn to cross-entropy and Kullback-Leibler divergence, which measure the misalignment between an estimated probability distribution \(q\) and a baseline or true distribution \(p\).</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="section-quantifying-misalignment"><a class="anchor" href="#section-quantifying-misalignment"></a><a class="link" href="#section-quantifying-misalignment">3.3. 🥢 Quantifying Misalignment: Cross-Entropy and KL Divergence</a></h3>
<div class="paragraph">
<p>Cross-entropy and Kullback-Leibler divergence (KL divergence) are two related concepts in information theory that measure the difference between two probability distributions. They are particularly useful in machine learning, especially in classification tasks, where we often need to compare a predicted distribution with a true distribution.</p>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>[KL divergence is] the amount we can win from a casino game, if we know the true game distribution is \(p\) but the house incorrectly believes it to be \(q\).</p>
</div>
</blockquote>
<div class="attribution">
&#8212; Research Engineer Callum McDougall
</div>
</div>
<div class="sect3">
<h4 id="_cross_entropy_measuring_the_average_misalignment"><a class="anchor" href="#_cross_entropy_measuring_the_average_misalignment"></a><a class="link" href="#_cross_entropy_measuring_the_average_misalignment">3.3.1. Cross-Entropy: Measuring the Average Misalignment</a></h4>
<div class="paragraph">
<p>One way to measure misalignment between distributions is through cross-entropy, a concept which may be considered entropy&#8217;s twin. They&#8217;re mostly fraternal, similar but not identical, and only rarely identical. 👭 (That&#8217;s probably as far as this analogy goes…)</p>
</div>
<div class="paragraph">
<p>Formally, given a categorical random variable \(X\), with \(c\) possible outcomes \(x_i\) , \(i\in\{1…c\}\), each with
a true probability \(p_{X}(x_{i})\) and predicted one of \(q_{X}(x_{i})\), the cross-entropy \(H_{X}(p,q)\) is:</p>
</div>
<div id="eq-cross-entropy" class="stemblock">
<div class="title">Cross-Entropy Equation</div>
<div class="content">
\[H_{X}(p,q) =  \sum_{i=1}^{c} p_{X}(x_{i}) h_{q(X)}(x_{i}) = -\sum_{i=1}^{c} p_{X}(x_{i}) \log_{2} q_{X}(x_{i})\]
</div>
</div>
<div class="paragraph">
<p>You&#8217;ll immediately notice that the <a href="#eq-cross-entropy">Cross-Entropy Equation</a>  closely resembles the <a href="#eq-entropy">Entropy Equation</a>, except we&#8217;ve swapped the self-information term, \(h_{p(X)}=-\log_2(p_X(x))\), with \(h_{q(X)}=-\log_2(q_X(x))\).</p>
</div>
<div class="paragraph">
<p>The intuition remains similar but accounting for true vs. predicted distributions:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>\(h_{q(X)(x)}\) represents the <strong><em>predicted</em></strong> self-information for each event \(x\).</p>
</li>
<li>
<p>\(p_{X}(x)\) serves as the weighting factor, reflecting the <strong><em>true</em></strong> distribution of events.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Cross-entropy quantifies the average amount of information needed to encode events from the true distribution \(p_{X}(x)\) using the predicted distribution \(q_{X}(x)\).<sup class="footnote">[<a id="_footnoteref_5" class="footnote" href="#_footnotedef_5" title="View footnote.">5</a>]</sup> In other words it is a measure of how well the predicted distribution aligns with the true distribution.</p>
</div>
<div class="paragraph">
<p>Since its a derivative of entropy, i.e, an aggregation of the self-information, its units of measure are bits (or nats) depending on the logarithm base used.</p>
</div>
<div class="paragraph">
<p>As cross-entropy involves two distributions it has unique properties that entropy does not. Two of the most important are:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Non Symmetric</strong>: By definition it&#8217;s asymmetric \(H_{X}(p,q) \ne H_{X}(q,p)\).</p>
</li>
<li>
<p><strong><em>Gibbs' inequality</em></strong>: The cross-entropy is always greater or equal to the entropy of the baseline distribution \(H_{X}(p,q) \ge H_{X}(p)\). This is a result of the properties of log functions.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In the special cases in which the predicted distribution is exactly equal to the true distribution, the cross-entropy equals the entropy of the true distribution: \(H_{X}(p,q=p) = H_{X}(p)\). According to Gibbs' inequality this is the lowest value cross-entropy can get.</p>
</div>
<div class="paragraph">
<p>In most cases where the \(q\) distributions deviate from the \(p\) distribution Gibbs' inequality manifests in the cross-entropy being greater than the entropy of the true distribution. This is a key property that makes cross-entropy useful in machine learning, particularly in classification tasks where we want to minimize the difference between predicted and true distributions and we will revisit this in the context of loss functions in section (TBD).</p>
</div>
<div class="paragraph">
<p>To gain further intuition into cross-entropy we next discuss the KL divergence which relates it to entropy.</p>
</div>
</div>
<div class="sect3">
<h4 id="section-kl-divergence"><a class="anchor" href="#section-kl-divergence"></a><a class="link" href="#section-kl-divergence">3.3.2. KL Divergence: Measuring the Information Loss</a></h4>
<div class="paragraph">
<p>The KL divergence, or <strong><em>relative entropy</em></strong>, is a measure of how much information is lost when using the predicted distribution \(q_X\) instead of the true distribution \(p_X\). It is defined as the difference between the cross-entropy and the entropy of the true distribution.</p>
</div>
<div class="paragraph">
<p>Formally: given a categorical random variable \(X\), with \(c\) possible outcomes \(x_i\) , \(i\in\{1…c\}\), each with
a true probability \(p_{X}(x_{i})\) and predicted one of \(q_{X}(x_{i})\), the KL divergence \(D_{\rm KL}(p_X||q_X)\) is:</p>
</div>
<div id="eq-kl-divergence" class="stemblock">
<div class="title">KL Divergence Equation</div>
<div class="content">
\[D_{\rm{KL}}(p_X||q_X) \equiv H_{X}(p,q) - H_{X}(p) = -\sum_{i=1}^{c} p_{X}(x_{i}) \log_{2} \frac{q_{X}(x_{i})}{p_{X}(x_{i})}\]
</div>
</div>
<div class="paragraph">
<p>To build intuition, we examine in <a href="#fig-bernoulli-kld">Figure 5</a> the Bernoulli case (\(c=2\)), similar to <a href="#fig-bernoulli-entropy">Figure 4</a> but for a few types of coins with underlying true probability distributions of:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A fair coin: \(p_X = [0.5,0.5]\) (dashed red line).</p>
</li>
<li>
<p>A partially loaded coin: \(p_X = [0.7,0.3]\) (dotted black).</p>
</li>
<li>
<p>A fully loaded coin: \(p_X = [1,0]\) (solid orange).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For all cases we will vary the predicted distribution continuously from \(q_X = [1,0]\) (e.g, always TAILS) to \(q_X = [0,1]\) (always HEADS), and calculate the KL divergence for each case.</p>
</div>
<div id="fig-bernoulli-kld" class="imageblock text-center">
<div class="content">
<img src="images/chapter-03-info-theory/bernoulli_kl_divergence.png" alt="KL Divergence Bernoulli Curves" width="600" height="400">
</div>
<div class="title">Figure 5. KL Divergence Bernoulli Curves</div>
</div>
<div class="paragraph">
<p>From this graph we can draw many insights into KL divergence many of which are mutual with cross-entropy:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Minimum Value</strong>: The minimum value occurs when the predicted distribution equals the true distribution \(q_X = p_X\). In this case the KL divergence is zero \(D_{\rm KL}(p_X||q_X=p_X)=0\) (as seen in the graph for all three lines), and for the cross-entropy it equals the entropy \(H(p_X||q_X=p_X)=H(p_X)\) (not in this graph).</p>
</li>
<li>
<p><strong>Non negative</strong>: More generally, when reformulating Gibbs' inequality, the KL divergence is shown to always be greater or equal to zero \(D_{\rm KL}(p_X, q_X) \ge 0\).</p>
</li>
<li>
<p><strong>Penalising deviations</strong>: Deviations from the truth causes a sharp rise in the cross-entropy and hence the KL divergence.</p>
</li>
<li>
<p><strong>Non Symmetric</strong>: By definition both cross-entropy and KL divergence are asymmetric when exchanging \(p_X\) and \(q_X\) \(D_{\rm KL}(p_X, q_X) \neq D_{\rm KL}(q_X, p_X)\). This means that the penalty around the truth distribution is assymetric (e.g, black dotted aline). Interestingly, also by definition, the penalty for deviations is symmetric only if the truth is a fair coin (red dashed line)..</p>
</li>
<li>
<p><strong>Units of measure</strong>: Its units of measure are also bits (or nats) depending on the logarithm base used.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In the context of application to machine learning it is useful to highlight the case of a fully loaded coin (solid orange line). You may notice that the KL divergence has a similar shape seen before in the self-information graph in <a href="#fig-self-information">Figure 3</a>. This is because, in the fully loaded case, the KL divergence reduces to the self-information of the true label&#8217;s predicted probability. Let&#8217;s call it \(q(x^*)\) for simplicity where \(x^*\) is the true class.</p>
</div>
<div class="paragraph">
<p>In other words, by plugging in, e.g,
\(p_X = [1,0]\), where the first class is the true one, into <a href="#eq-kl-divergence">KL Divergence Equation</a> we see that the KL divergence simplifies to the cross-entropy which further simplifies to the self-information of \(q_X\) as follows:</p>
</div>
<div class="paragraph">
<p>\(D_{\rm KL}(p_X = [1,0]||q_X) = H(p_X = [1,0]||q_X) = -\log_{2}(q_X)\).</p>
</div>
<div class="paragraph">
<p>This property is what makes KL divergence so useful as a classifier loss function when labels are one-hot encoded - the machine learning equivalent of a fully loaded coin (or die) in the base of a binary (or multi-class) classifier: a vector of length equal to the number of classes, with all entries set to 0 except the true class \(x^*\) set to 1. Importantly, this applies not only to binary classifiers but generalizes to any number of classes \(c \ge 2\). For a hands-on discussion of this application, see the <em>KL Divergence as a Classifier Loss Function</em> notebook (link TBD).</p>
</div>
</div>
<div class="sect3">
<h4 id="_kl_divergence_interpretation"><a class="anchor" href="#_kl_divergence_interpretation"></a><a class="link" href="#_kl_divergence_interpretation">3.3.3. KL Divergence Interpretation</a></h4>
<div class="paragraph">
<p>Interpretation of KL divergence depends on the context:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Statistics</strong>: The KL divergence is the expected value of the log of the ratio of two distributions. This is considered the most powerful way to distinguish between two distributions.<sup class="footnote">[<a id="_footnoteref_6" class="footnote" href="#_footnotedef_6" title="View footnote.">6</a>]</sup></p>
</li>
<li>
<p><strong>Coding</strong>: In coding theory the KL divergence quantifies the expected number of extra bits required to code samples from the true distribution \(p\) when using a code optimized for the predicted distribution \(q\) instead of the true distribution \(p\). This interpretation follows directly from the identity that KL is the difference between cross-entropy and entropy.</p>
</li>
<li>
<p><strong>Bayesian Inference</strong>: The KL divergence is used to express the information gain from updating one&#8217;s beliefs from a prior distribution \(q\) to a posterior distribution \(p\) after observing data.</p>
</li>
<li>
<p><strong>Machine Learning Inference</strong>: The KL divergence is considered the <em>information gain</em> that may be achieved when using distribution \(p\) instead of a currently used one \(q\). This is used in the context of decision tree learning, where the KL divergence is used to measure the information gain from splitting a node into child nodes based on a feature. This is useful for selecting the best feature to split on at each node in the tree.</p>
</li>
<li>
<p><strong>Information Geometry</strong>: The KL divergence is used to measure the distance between two probability distributions in a geometric space. Here \(p\) typically represents the true distribution and \(q\) an approximation chosen to make computation feasible. Applications include clustering and dimensionality reduction.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For a hands-on discussion of applications to some of these topics, see notebooks titled <em>Categorical KL Divergence Applications</em> (link TBD) and <em>Continuous Information Applications</em> (link TBD). <sup class="footnote">[<a id="_footnoteref_7" class="footnote" href="#_footnotedef_7" title="View footnote.">7</a>]</sup></p>
</div>
<div class="paragraph">
<p>To summarise, KL divergence captures the gap between cross-entropy and entropy. When entropy is fixed, monitoring cross-entropy or KL divergence is equivalent.</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
So, when should we use cross-entropy vs. KL divergence? In systems where normalization of baseline entropy is not a concern, such as in classifier loss functions, KL divergence and cross-entropy are effectively interchangeable. However, KL divergence has a lower bound of zero, which can make it slightly easier to interpret.
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="_cross_entropy_and_kl_divergence_in_python"><a class="anchor" href="#_cross_entropy_and_kl_divergence_in_python"></a><a class="link" href="#_cross_entropy_and_kl_divergence_in_python">3.3.4. Cross-Entropy and KL Divergence In Python 🐍</a></h4>
<div class="paragraph">
<p>Oddly in Python there is no function to calculate cross-entropy, just KL divergence! My best guess is due to the fact that these are for the most part interchangeable when the baseline entropy is fixed.</p>
</div>
<div class="paragraph">
<p>For example, as far as I can tell, when using the <code>scipy.stats.entropy</code> function the KL divergence is a one step calculation where cross-entropy requires two steps (unless you write it from scratch):</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>entropy(p)</code>: entropy of <code>p</code></p>
</li>
<li>
<p><code>entropy(p,q)</code> : KL divergence of baseline <code>p</code> and prediction <code>q</code>:</p>
</li>
<li>
<p><code>entropy(p) + entropy(p,q)</code>: cross-entropy of baseline <code>p</code> and prediction <code>q</code>, which may also be calculated using <code>-sum(p*log(q))</code> . (Note that the former handles zero values well, while the latter does not.)</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="section-kld-not-metric"><a class="anchor" href="#section-kld-not-metric"></a><a class="link" href="#section-kld-not-metric">3.3.5. KL Divergence is a Distance but not a Metric</a></h4>
<div class="paragraph">
<p>KL Divergence is considered a <em>statistical distance</em>, i.e, a measure of how different two probability distributions are by statistical dependence. However, it does not satisfy all the properties required to be classified as a true metric in the mathematical sense.</p>
</div>
<div class="paragraph">
<p>In order for a function to be considered a metric, it must satisfy a few properties:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>✅  Non-negativity: \(D_{\rm KL}(p,q) \ge 0\) (Gibbs' inequality)</p>
</li>
<li>
<p>✅  Identity of indiscernibles: \(D_{\rm KL}(p,q) = 0\) if and only if \(p = q\)</p>
</li>
<li>
<p>❌ Symmetry: Does not apply because: \(D_{\rm KL}(p,q) \ne D_{\rm KL}(q,p)\)</p>
</li>
<li>
<p>❌ Triangle inequality: For a metric \(f\) this requires \(f(p,q) + f(q,r) \ge f(p,r)\)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>KL divergence satisfies the first two properties, but fails the last two. It is not symmetric, and it does not satisfy the triangle inequality. This means that KL divergence is not a metric in the mathematical sense.</p>
</div>
<div class="paragraph">
<p>There are a few symmetric variants of KL divergence that satisfy the metric criteria, most notably:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Jeffreys divergence</strong> (also known as Population Stability Index, or PSI in Banking and Finance): is the average of two KL divergences when swapping the distributions: \(D_{\rm J}(p,q) = D_{\rm KL}(p,q) + D_{\rm KL}(q,p)\).</p>
</li>
<li>
<p><strong>Jensen-Shannon divergence</strong> (JS divergence): \(D_{\rm JS}(p,q) = \frac{1}{2} D_{\rm KL}(p,m) + \frac{1}{2} D_{\rm KL}(q,m)\) where the <strong><em>mixture distribution</em></strong> \(m=\frac{1}{2}(p+q)\) is the average distribution.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Whereas these have the advantage of being symmetric (and JS divergences also being bounded between 0 and 1), they are not as popular as KL divergence due to disadvantages.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Less interpretable: Jeffreys and JS divergences do not have the same direct relationship to cross-entropy and entropy and by association to probability likelihoods. This makes them less natural for probabilistic modeling.</p>
</li>
<li>
<p>Machine learning utility: JS divergence can yield weak or vanishing gradients and Jeffreys is not defined when using one-hot vectors. This makes them less suitable for use as loss functions in machine learning.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>If you are keen to learn about their symmetric properties and potential usage, in the  <em>KL Divergence Variants</em> notebook (link TBD) I compare between them and provide hands-on python implementations.</p>
</div>
</div>
<div class="sect3">
<h4 id="_quantifying_misalignement_summary"><a class="anchor" href="#_quantifying_misalignement_summary"></a><a class="link" href="#_quantifying_misalignement_summary">3.3.6. Quantifying Misalignement Summary</a></h4>
<div class="paragraph">
<p>Cross-entropy and KL divergence are powerful tools for quantifying the misalignment between two probability distributions. They are particularly useful in machine learning. A primary example is classification tasks, where the goal is to minimize the gap between predicted probabilities and true label distributions (see section TBD).</p>
</div>
<div class="paragraph">
<p>Beyond training, KL divergence is also valuable for model monitoring, where it can detect changes in data distributions over time. In the categorical case, this might mean tracking shifts in class proportions (e.g., spam vs. non-spam). In the continuous case of KL divergence (which we cover in <a href="#section-continuous-kld">Section 3.5.2</a>), it often involves monitoring input or feature distributions, using binning or density estimation to make the comparison feasible. In both settings, KL divergence between incoming data and a baseline helps flag when a model may need retraining. (For a hands-on example, see KL for <em>Model Monitoring</em> notebook, link TBD.)</p>
</div>
<div class="paragraph">
<p>It&#8217;s also worth noting the deeper interpretation: cross-entropy measures the average information needed to encode events from the true distribution using the predicted one, while KL divergence captures the extra cost compared to the optimal encoding. Fully exploring this connection requires ideas from data compression, which are beyond the scope of this book, but the <em>Message Length Optimization</em> notebook (link TBD) offers a hands-on introduction to this fascinating topic.</p>
</div>
<div class="paragraph">
<p>So far, we&#8217;ve focused on surprise (self-information), uncertainty (entropy) and misalignment (KL divergence) within a single variable&#8217;s distribution. But what if we want to capture the shared structure between two variables? That&#8217;s where mutual information comes in: a measure of how much knowing one variable reduces uncertainty about the other.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="section-quantifying-gain"><a class="anchor" href="#section-quantifying-gain"></a><a class="link" href="#section-quantifying-gain">3.4. 🤝 Quantifying Gain</a></h3>
<div class="paragraph">
<p><strong><em>Mutual information</em></strong> (MI) extends the ideas of self-information, entropy, and KL divergence by shifting the focus from a single variable to the relationship between two. It measures how much knowing one variable reduces uncertainty about the other, making it a natural way to quantify dependence. This is especially valuable in data science and machine learning, where uncovering such relationships helps with tasks like feature selection, clustering, and representation learning.</p>
</div>
<div class="paragraph">
<p>Put simply, MI measures the dependence between two variables. If two variables are independent, their mutual information is zero. If they are strongly dependent, the mutual information grows accordingly.</p>
</div>
<div class="paragraph">
<p>There are several equivalent ways to express it mathematically which we will explore in <a href="#section-mutual-information">Section 3.4.4</a>.
Briefly for two variables \(X,Y\) with classes \(x \in X, y \in Y\)
a commonly used equation is:</p>
</div>
<div id="eq-mi-log-ratio" class="stemblock">
<div class="title">Mutual Information Equation</div>
<div class="content">
\[I(X;Y) = \sum_{y \in Y}\sum_{x \in X} p_{X,Y}(x,y) \log\left(\frac{p_{X,Y}(x,y)}{p_X(x)p_Y(y)}\right)\]
</div>
</div>
<div class="paragraph">
<p>The goal of this section is to help you become comfortable reading and interpreting this <a href="#eq-mi-log-ratio">Mutual Information Equation</a> by unpacking its components. To do so we will recall insights from the  Joint and Marginal Probability section <a href="#joint-marginal-probs">Section 2.3.1</a> and discuss the most fundamental building block of mutual information: the point mutual information (PMI).</p>
</div>
<div class="paragraph">
<p>To do so, we first need to define self-information for joint events and discuss Shannon&#8217;s third axiom.</p>
</div>
<div class="sect3">
<h4 id="_self_information_of_joint_events"><a class="anchor" href="#_self_information_of_joint_events"></a><a class="link" href="#_self_information_of_joint_events">3.4.1. Self-Information of Joint Events</a></h4>
<div class="paragraph">
<p>Self-information for joint events is defined similarly to that of individual events, but it takes into account the joint probability distribution. For two random variables \(X\) and \(Y\), the self-information of their joint event \((x,y)\) is given by:</p>
</div>
<div id="eq-joint-self-information" class="stemblock">
<div class="title">Joint Self-Information Equation</div>
<div class="content">
\[h_{X,Y}(x,y) = -\log_2(p_{X,Y}(x,y))\]
</div>
</div>
<div class="paragraph">
<p>This <strong><em>joint self-information</em></strong> captures the surprise, or amount of information gained, by observing the specific joint outcome \((x,y)\).</p>
</div>
<div class="paragraph">
<p>In that sense the self-information \(h_{X}(x)\) and \(h_{Y}(y)\) of individual events \(x\) and \(y\) that we discussed in <a href="#self-information">Section 3.1.1</a> are special cases of the joint self-information when considering marginal probabilities.</p>
</div>
<div class="paragraph">
<p>Their combination \(h_{X}(x) + h_{Y}(y)\) represents the total self-information of observing both events independently, which we will call the <strong><em>independent joint self-information</em></strong>.</p>
</div>
<div class="paragraph">
<p>To make sense of this, let&#8217;s return to the craps game example from <a href="#joint-marginal-probs">Section 2.3.1</a>, where we tossed two dice that are independent and identically distributed (iid). The joint event \((x=1,y=1)\) (snake eyes) has a joint probability of \(p_{X,Y}(1,1)=\frac{1}{36}\), since this is one of 36 possible outcomes with equal probabilitiy. The self-information of this joint event is:</p>
</div>
<div class="stemblock">
<div class="content">
\[h_{X,Y}(1,1) = -\log_2(p_{X,Y}(1,1)) = -\log_2\left(\frac{1}{36}\right) = 5.17 \text{ bits}\]
</div>
</div>
<div class="paragraph">
<p>This means that observing snake eyes provides 5.17 bits of information.</p>
</div>
<div class="paragraph">
<p>Let&#8217;s consider the individual events \(x=1\) and \(y=1\). Each has a marginal probability of \(p_X(1)=\frac{1}{6}\) and \(p_Y(1)=\frac{1}{6}\). The self-information for each individual event is:</p>
</div>
<div class="stemblock">
<div class="content">
\[\begin{align*}
h_{X}(1) &amp;= -\log_2(p_{X}(1)) = -\log_2\left(\frac{1}{6}\right) = 2.58 \text{ bits} \\
h_{Y}(1) &amp;= -\log_2(p_{Y}(1)) = -\log_2\left(\frac{1}{6}\right) = 2.58 \text{ bits}
\end{align*}\]
</div>
</div>
<div class="paragraph">
<p>The total self-information of observing both events independently is:</p>
</div>
<div class="stemblock">
<div class="content">
\[h_{X}(1) + h_{Y}(1) =  2.58 + 2.58 = 5.17 \text{ bits}\]
</div>
</div>
<div class="paragraph">
<p>This means that observing both events independently provides a total of 5.17 bits of information.
This matches the joint self-information calculated earlier, confirming that for independent events, the joint self-information equals the sum of the individual self-information values.</p>
</div>
<div class="paragraph">
<p>This property is not a coincidence but rather a direct consequence of Shannon&#8217;s third axiom, which we will discuss next.</p>
</div>
</div>
<div class="sect3">
<h4 id="shannon-third-axiom"><a class="anchor" href="#shannon-third-axiom"></a><a class="link" href="#shannon-third-axiom">3.4.2. The Third Axiom: Additivity of Independent Events</a></h4>
<div class="paragraph">
<p>The third axiom upon which Shannon devised self-information is that the total information from independent events should be additive.</p>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>If two independent events are measured separately, the total amount of information is the sum of the self-information of each event.</p>
</div>
</blockquote>
<div class="attribution">
&#8212; Third Axiom of Information Theory
</div>
</div>
<div class="paragraph">
<p>Living in the midst of the telecommunications boom, Claude Shannon observed that communication metrics <em>“tend to vary linearly with the logarithm of the number of possibilities.”</em> This observation led him to leverage the additivity property of logarithms, which aligned naturally with the behaviour he sought in an information measure.</p>
</div>
<div class="paragraph">
<p>In mathematics, the term additive depends on context. Here we say a function \(f\) is additive if \(f(x,y)=f(x)+f(y)\) for all elements \(x\) and \(y\) if they are independent.</p>
</div>
<div class="paragraph">
<p>In the case of information, the function of interest is self-information \(h(.)=–\log_2(p(.))\).</p>
</div>
<div class="paragraph">
<p>Assuming \(x\) and \(y\) are i.i.d. events, using what we&#8217;ve learned about <a href="#joint-marginal-probs">joint and marginal probabilities</a>, we can relate the self-information of the joint event \((x,y)\) to the self-information of each marginal:</p>
</div>
<div id="eq-joint-self-information-independent" class="stemblock">
<div class="title">Joint Self-Information of Independent Events</div>
<div class="content">
\[\begin{align*}
h_{X,Y}(x,y) &amp;= -\log_2(p_{X,Y}(x,y)) \\ &amp;=^{{\rm let } x\perp y} -\log_2(p_X(x)p_Y(y)) \\ &amp;= -\log_2(p_X(x)) - \log_2(p_Y(y)) \\ &amp;= h_{X}(x) + h_{Y}(y)
\end{align*}\]
</div>
</div>
<div class="paragraph">
<p>The \(h_{X,Y}(x,y) =  h_{X}(x) + h_{Y}(y)\) equation is a direct manifestation of Shannon&#8217;s third axiom: the total information of two independent events occurring together is the sum of their individual information values.</p>
</div>
<div class="paragraph">
<p>But what if \(x\) and \(y\) are dependent events?</p>
</div>
</div>
<div class="sect3">
<h4 id="section-pmi"><a class="anchor" href="#section-pmi"></a><a class="link" href="#section-pmi">3.4.3. Point Mutual Information</a></h4>
<div class="paragraph">
<p>A more interesting and often more useful case is when events \(x\) and \(y\) are dependent, meaning they share some overlapping (mutual) information.</p>
</div>
<div class="paragraph">
<p>This is addressed by the <strong><em>point mutual information</em></strong> (PMI) (or pointwise mutual information).</p>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>[PMI is] one of the most important concepts in Natural Language Processing</p>
</div>
</blockquote>
<div class="attribution">
&#8212; Jurafsky & Martin (2021)
</div>
</div>
<div class="paragraph">
<p><a href="#jurafsky_martin2021">[jurafsky_martin2021]</a></p>
</div>
<div class="paragraph">
<p>When events are dependent, the additivity equation no longer holds: \(h_{X,Y}(x,y) \neq  h_{X}(x) + h_{Y}(y)\).</p>
</div>
<div class="paragraph">
<p>This discrepancy motivates the definition of an additional quantity: the PMI, which was first introduced by Robert Fano in 1961, thirteen years after Shannon&#8217;s breakthrough paper on information theory.</p>
</div>
<div class="paragraph">
<p>It captures how much information is shared between the specific events \(x\) and \(y\). Rearranging the inequality, we define:</p>
</div>
<div id="eq-pmi" class="stemblock">
<div class="title">PMI Equation</div>
<div class="content">
\[\begin{align*}
{\rm pmi}_{X,Y}(x;y) &amp;= h_{X}(x) + h_{Y}(y) - h_{X,Y}(x,y) \\ &amp;= \log_2\left(\frac{p_{X,Y}(x,y)}{p_X(x)p_Y(y)}\right)
\end{align*}\]
</div>
</div>
<div class="paragraph">
<p>In the middle equation we subtract the joint self-information \(h_{X,Y}(x,y)\) from the independent joint self-information \(h_{X}(x)+h_{Y}(y)\).</p>
</div>
<div class="paragraph">
<p>The intuition here is that PMI measures how surprising the co-occurrence is, relative to a baseline of independence.</p>
</div>
<div class="paragraph">
<p>The equation on the right is the result of substituting the definition of self-information terms
with probabilities \(h(.) = -log₂(p(.))\).</p>
</div>
<div class="paragraph">
<p>Does the right side of the <a href="#eq-pmi">PMI Equation</a> look familiar? That&#8217;s because you&#8217;ve seen it earlier in the definition of mutual information \(I(X;Y)\) in the <a href="#eq-mi-log-ratio">Mutual Information Equation</a>. In <a href="#section-mutual-information">Section 3.4.4</a>, we&#8217;ll make the connection explicit by showing that MI is the expected value, i.e. the average, of PMI across all joint events \((x,y)\).</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
Why the Semicolon? You might have noticed that in both the <a href="#eq-mi-log-ratio">Mutual Information</a> and <a href="#eq-pmi">PMI</a> equations, we often use a semicolon ";" to separate the symbols for events \((x, y)\) or variables \((X, Y)\), rather than a comma ",", which is usually used to list symbols. The semicolon helps visually separate the two informational roles, one variable being observed, the other gaining information, and avoids confusion when dealing with groups of variables.
</td>
</tr>
</table>
</div>
<div class="sect4">
<h5 id="_pmi_properties"><a class="anchor" href="#_pmi_properties"></a><a class="link" href="#_pmi_properties">PMI Properties</a></h5>
<div class="paragraph">
<p>The main properties of PMI as an information metric:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Variable Symmetry</strong>: It&#8217;s symmetric when changing \(x\) and \(y\) since \(p_{X,Y}(x,y)=p_{X,Y}(y,x)\). E.g., if in a Market Basket Analysis where one observes the co-occurrence of items bought together, say bread 🍞 and milk 🥛, then pmi(🍞;🥛)=pmi(🥛;🍞) due to the symmetry of joint probabilities.</p>
</li>
<li>
<p><strong>Relative information</strong>: while the joint self-information \(h_{X,Y}(x,y)\) expresses the absolute information of the joint event \((x,y)\), the PMI expresses how much it differs from the combined information of the marginals, \(h_{X}(x)+h_{Y}(y)\).</p>
</li>
<li>
<p><strong>Units of measure</strong>: is still bits (or nots depending on the log base) since PMI is a subtraction of self-information.</p>
</li>
<li>
<p><strong>Unbounded</strong>: due to PMI being a logarithm of a ratio of probabilities \(p_{X,Y}(x,y) / ( p_{X}(x)p_{Y}(y) )\) its range is broad. If the probability ratio is larger than one PMI will be positive; if the probability below one PMI is negative. We will later discuss gaining an intuition for different values.</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="_pmi_from_conditional_information"><a class="anchor" href="#_pmi_from_conditional_information"></a><a class="link" href="#_pmi_from_conditional_information">PMI from Conditional Information</a></h5>
<div class="paragraph">
<p>One of the most useful interpretations comes from rewriting PMI using conditional probabilities. Starting from the right hand side of <a href="#eq-pmi">PMI Equation</a> we can apply the <a href="#section-chain-rule">chain rule</a> of probability \(p_{X,Y}(x,y) = p_{X|Y}(x|y) \cdot p_{Y}(y)\) to obain:</p>
</div>
<div id="eq-pmi-conditional" class="stemblock">
<div class="title">PMI Equation using Conditional Probablity</div>
<div class="content">
\[{\rm pmi}_{X,Y}(x;y) = \log_2\left(\frac{p_{X|Y}(x|y)}{p_X(x)}\right)\]
</div>
</div>
<div class="paragraph">
<p>Note that I have arbitrarily chosen to condition on \(y\). But due to symmetry, when conditioning on \(x\) we obtain a similar result.</p>
</div>
<div class="paragraph">
<p>To interpret <a href="#eq-pmi-conditional">PMI Equation using Conditional Probablity</a> let&#8217;s examine the specific case of
\({\rm pmi}_{X,Y}(x;y)=1\) bit which manifests from \({p_{X|Y}(x|y)=2p_X(x)}\).</p>
</div>
<div class="paragraph">
<p>This may be read as: when PMI is 1 bit that means that knowing one event (e.g, \(y\)) that makes the other (\(x\)) twice as likely than not knowing.</p>
</div>
<div class="paragraph">
<p>In other words, by conditioning on event \(y\) (\({p_{X|Y}(x|y)}\)) we double the probability of \(x\) compared to using the marginal probability \({p_X(x)}\). This is the interpretive power of PMI: it quantifies how much our belief in one event changes due to knowledge of another. In section TBD we will discuss this being considered a <em>Bayesian</em> mindset.</p>
</div>
<div class="paragraph">
<p>Being mindful of the power of <a href="#section-conditional-probs">Conditional Probabilities</a> as a key concept to describe
dependency between variables (and as we&#8217;ve seen here, gain) it is useful to express conditional self information:</p>
</div>
<div id="eq-conditional-selfinfo" class="stemblock">
<div class="title">Conditional Self Information</div>
<div class="content">
\[h_{X|Y}(x|y) = -\log_2(p_{X|Y}(x|y))\]
</div>
</div>
<div class="paragraph">
<p>From <a href="#eq-conditional-selfinfo">Conditional Self Information</a> and <a href="#eq-pmi-conditional">PMI Equation using Conditional Probablity</a> we can easily relate
PMI to conditional self-information:</p>
</div>
<div id="eq-pmi-selfinfo-conditional" class="stemblock">
<div class="title">PMI by Conditional Self Information</div>
<div class="content">
\[{\rm pmi}_{X,Y}(x;y) = h_X(x) - h_{X|Y}(x|y)\]
</div>
</div>
<div class="paragraph">
<p>In <a href="#section-mutual-information">Section 3.4.4</a> we will further demonstrate how useful the notion of conditional
self-information is to understand mutual information.</p>
</div>
<div class="paragraph">
<p>Beforehand we examine two more insightful cases, e.g, when PMI is 0 bits. In this case \({p_{X|Y}(x|y)=p_X(x)}\) which may be read as: knowing \(y\) does not change the probability of \(x\). This is the hallmark of independence between events \(x\) and \(y\).</p>
</div>
<div class="paragraph">
<p>Finally, when saying that PMI is unbounded we refer also to the possibility of negative values.</p>
</div>
<div class="paragraph">
<p>To see this let&#8217;s examine the opposite case where PMI is -1 bit which manifests from \({p_{X|Y}(x|y)=\frac{1}{2}p_X(x)}\). This may be read as: when PMI is -1 bit that means that knowing one event (e.g, \(y\)) that makes the other (\(x\)) half as likely than not knowing.</p>
</div>
<div class="paragraph">
<p>This may come off as counter intuitive: how can knowing \(y\) make \(x\) less likely? The answer is that this is possible when \(x\) and \(y\) are negatively correlated. For example, in a medical diagnosis context, if \(x\) represents having a certain disease and \(y\) represents testing negative for it, then knowing \(y\) (the negative test result) makes \(x\) (having the disease) less likely. Another way of looking at this is reexamining <a href="#eq-pmi">PMI Equation</a> where we realise that this just means that the joint probability \(p_{X,Y}(x,y)\) is less than the product of the marginals \(p_X(x)p_Y(y)\), which is a sign of negative correlation. We further explore this concept in the <em>Co-occurrence Matrices in Market Basket Analysis</em> notebook (link TBD).</p>
</div>
<div class="paragraph">
<p><a href="#tbl-pmi-interpretation">Table 1</a> summarises these interpretations for a few PMI example values.</p>
</div>
<table id="tbl-pmi-interpretation" class="tableblock frame-all grid-all stretch">
<caption class="title">Table 1. PMI Example Values and Their Interpretations</caption>
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">PMI Value</th>
<th class="tableblock halign-left valign-top">Condition</th>
<th class="tableblock halign-left valign-top">Interpretation</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">pmi = +1 bit</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(p_{X|Y}(x|y)=2p_X(x)\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Knowing y doubles the probability of x</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">pmi = -1 bit</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(p_{X|Y}(x|y)=\frac{1}{2}p_X(x)\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Knowing y halves the probability of x</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">pmi = 0 bits</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(p_{X|Y}(x|y)=p_X(x)\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Knowing y does not change the probability of x (independence)</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="sect3">
<h4 id="section-mutual-information"><a class="anchor" href="#section-mutual-information"></a><a class="link" href="#section-mutual-information">3.4.4. Mutual Information Definition</a></h4>
<div class="paragraph">
<p>Mutual Information quantifies how much knowing one variable tells us about another. In other words</p>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>Mutual Information is the average amount of information gained about one variable by observing the other.</p>
</div>
</blockquote>
</div>
<div class="paragraph">
<p>It captures the mutual dependence between two variables and can be interpreted as the expected reduction in uncertainty about one variable after learning the value of the other.</p>
</div>
<div class="paragraph">
<p>This reduction in uncertainty may be also considered gain in certainty.</p>
</div>
<div class="paragraph">
<p>There are several ways to arrive at this idea, depending on your starting point. In this section, we&#8217;ll walk  through two complementary perspectives. Think of them as different trails leading up the same hill, each offering a unique but ultimately convergent, and miraculous, view of this concept.</p>
</div>
</div>
<div class="sect3">
<h4 id="_mutual_information_from_pmi"><a class="anchor" href="#_mutual_information_from_pmi"></a><a class="link" href="#_mutual_information_from_pmi">Mutual Information from PMI</a></h4>
<div class="paragraph">
<p>We start with the building block of information theory for multiple variables, <a href="#section-pmi">PMI</a>. We will use this to construct MI brick by brick 🧱.</p>
</div>
<div class="paragraph">
<p>Mutual information is simply the expected value of PMI across all possible event pairs. E.g, for events in spaces \({x} \in X\) and \({y} \in Y\) we have:</p>
</div>
<div id="eq-mi-from-pmi" class="stemblock">
<div class="title">MI from PMI Equation</div>
<div class="content">
\[\begin{align*}
I(X;Y) &amp;=  \sum_{y \in Y}\sum_{x \in X} p_{X,Y}(x,y) {\rm pmi}_{X,Y}(x;y) \\ &amp; =  \sum_{y \in Y}\sum_{x \in X} p_{X,Y}(x,y) \log\left(\frac{p_{X,Y}(x,y)}{p_X(x)p_Y(y)}\right)
\end{align*}\]
</div>
</div>
<div class="paragraph">
<p>The first equation mirrors other aggregates seen in the <a href="#eq-entropy">Entropy Equation</a> and KL divergence <a href="#eq-kl-divergence">KL Divergence Equation</a>:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>\({\rm pmi}_{X,Y}(x;y)\): captures the local information gain for a pair of events \((x, y)\).</p>
</li>
<li>
<p>\(p_{X,Y}(x,y)\): the joint probability acts as a weighting factor.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The second equation is obtained by substituting the definition of PMI (<a href="#eq-pmi-conditional">PMI Equation using Conditional Probablity</a>) into the first equation and obtain the familiar <a href="#eq-mi-log-ratio">Mutual Information Equation</a>.</p>
</div>
<div class="paragraph">
<p>From these equations emerge key properties of mutual information:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>It&#8217;s symmetric when changing \({X}\) and \({Y}\).</p>
</li>
<li>
<p>It&#8217;s invariant to reparameterisation of the data. E.g, Imagine that \({X}\) is in miles and you&#8217;d like to convert it to km \({X} \rightarrow 1.6 {X}\). The MI will not change. This is because each variable is both in the nominator and denominator of the log ratio.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The hawk eye may noticed <a href="#eq-mi-log-ratio">Mutual Information Equation</a> may be written rewritten much more elegantly in terms of KL divergence:</p>
</div>
<div id="eq-mi-kld" class="stemblock">
<div class="title">Mutual Information as KL Divergence Equation</div>
<div class="content">
\[\begin{align*}
I(X;Y) &amp;= \sum_{y \in Y}\sum_{x \in X} p_{X,Y}(x,y) \log\left(\frac{p_{X,Y}(x,y)}{p_X(x)p_Y(y)}\right) \\ &amp;= D_{KL}(p_{X,Y} \| p_X \otimes p_Y)
\end{align*}\]
</div>
</div>
<div class="paragraph">
<p>Here \( p_X \otimes p_Y\) is the independent joint probability, which is comprised of the outer product of the marginal distributions of \(X\) and \(Y\). In other words this means the “joint probability distribution if \(X\) and \(Y\) were independent”.</p>
</div>
<div class="paragraph">
<p>The intuition is that MI quantifies the divergence between the two joint probabilities: the residual information when subtracting the outer product from the actual joint probability.</p>
</div>
<div class="paragraph">
<p>Additional key properties that emerge from the KL divergence perspective are:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>MI is non-negative (recall Gibbs' inequality).</p>
</li>
<li>
<p>MI is not a metric in a mathematical sense as seen in <a href="#section-kld-not-metric">Section 3.3.5</a>. We address this later when we discuss <a href="#section-mi-variants">MI Variants</a>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>🤔 It&#8217;s worth pausing to discuss the symmetry property. On the one hand we said that MI is symmetric and on the other hand KL divergence is not. This apparent conflict is the aim of summary question 4 in <a href="#section-info-theory-summary">Section 3.6</a>. Try to see if you can resolve for this.</p>
</div>
<div class="paragraph">
<p>Now that we&#8217;ve reached a solid understanding of mutual information from the PMI perspective, you&#8217;ll be glad to know there&#8217;s another route to the same insight. This time a bit more direct and arguably more intuitive.</p>
</div>
</div>
<div class="sect3">
<h4 id="_mutual_information_from_entropy"><a class="anchor" href="#_mutual_information_from_entropy"></a><a class="link" href="#_mutual_information_from_entropy">Mutual Information from Entropy</a></h4>
<div class="paragraph">
<p>Perhaps the most intuitive way to describe MI as the information gain of one variable by knowing the other is by the following Venn Diagram of the entropies of two variables \(X\) and \(Y\):</p>
</div>
<div id="fig-entropy-venn" class="imageblock text-center">
<div class="content">
<img src="images/chapter-03-info-theory/mi_venn_entropy.png" alt="Entropy Venn Diagram" width="600" height="400">
</div>
<div class="title">Figure 6. Entropy Venn Diagram</div>
</div>
<div class="paragraph">
<p>Each circle symbolises the entropy of a parameter where \(H(X)\) is the left circle (with the green circumference), \(H(Y)\) is the right one (orange circumference) and their overlap purple region is the mutual information \(I(X;Y)\). Larger size indicates higher entropy (i.e, more uncertainty).</p>
</div>
<div class="paragraph">
<p>Other regions of interest are:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>\(H(X,Y)\): is the joint entropy (i.e, entropy of the joint probability) marked by the union of both circles (red+purple+blue).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>and two conditional entropies:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>\(H(X|Y)\): the remaining entropy of \(X\) when \(Y\) is known (red area).</p>
</li>
<li>
<p>\(H(Y|X)\): the remaining entropy of \(Y\) when \(X\) is known (blue area).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The joint entropy is defined as the average self-information of joint events
(see <a href="#eq-joint-self-information">Joint Self-Information Equation</a>):</p>
</div>
<div id="eq-joint-entropy" class="stemblock">
<div class="title">Joint Entropy Equation</div>
<div class="content">
\[\begin{align*}
H(X,Y) &amp;= \sum_{y \in Y}\sum_{x \in X} p_{X,Y}(x,y) h_{X,Y}(x,y) \\ &amp;= -\sum_{y \in Y}\sum_{x \in X} p_{X,Y}(x,y) \log(p_{X,Y}(x,y))
\end{align*}\]
</div>
</div>
<div class="paragraph">
<p>Note that this is the same equation as <a href="#eq-mi-pmi-path">[eq-mi-pmi-path]</a> but without the independent joint probability component (the \(p_X \otimes p_Y\)).</p>
</div>
<div class="paragraph">
<p>The conditional entropies \(H(.|.)\) have a similar structure but we replace the joint probability in the log to the relevant conditional on one, e.g,</p>
</div>
<div id="eq-conditioned-entropy" class="stemblock">
<div class="title">Conditioned Entropy Equation</div>
<div class="content">
\[H(X|Y) = -\sum_{y \in Y}\sum_{x \in X} p_{X,Y}(x,y) \log(p_{X|Y}(x|y))\]
</div>
</div>
<div class="paragraph">
<p>, where \(p_{X|Y}(x|y)=\frac{p_{X,Y}(x,y)}{p_Y(y)}\) according to the <a href="#section-chain-rule">Chain Rule</a>. Note that here the weighing factor is still the joint probability \(p_{X,Y}(x,y)\), not the conditional probability \(p_{X|Y}(x|y)\).</p>
</div>
<div class="paragraph">
<p>By examining all the components in the entropy Venn diagram, including the joint entropy \(H(X,Y)\), the marginal entropies \(H(X)\) and \(H(Y)\), and the conditional entropies \(H(X|Y)\) and \(H(Y|X)\), we see that MI can be expressed in several equivalent forms.</p>
</div>
<div class="paragraph">
<p>One of the most intuitive formulations is:</p>
</div>
<div id="eq-mi-from-entropy" class="stemblock">
<div class="title">MI from Entropy Equation</div>
<div class="content">
\[\begin{align*}
I(X;Y) &amp;= H(X) + H(Y) - H(X,Y) \\ &amp;= H(X,Y)_\perp - H(X,Y)
\end{align*}\]
</div>
</div>
<div class="paragraph">
<p>In plain terms, this means that mutual information \(I(X;Y)\) is the difference between the total uncertainty in \(X\) and \(Y\) separately, i.e., if they were independent) (captured by the sum \(H(X) + H(Y) \equiv H(X,Y)_\perp\) )  and their actual joint uncertainty H(X|Y).</p>
</div>
<div class="paragraph">
<p>The Entropy Venn diagram in <a href="#fig-entropy-venn">Figure 6</a>  helps visualise this <a href="#eq-mi-from-entropy">MI from Entropy Equation</a> and other related identities:</p>
</div>
<div id="eqs-mi-from-entropy" class="stemblock">
<div class="title">MI from Entropy Equations</div>
<div class="content">
\[\begin{align*}
I(X;Y) &amp;= H(X) + H(Y) - H(X,Y) \\
&amp;= H(X,Y) - H(X|Y) - H(Y|X) \\
&amp;= H(X) - H(X|Y) \\
&amp;= H(Y) - H(Y|X)
\end{align*}\]
</div>
</div>
<div class="paragraph">
<p>To build intuition, I suggest exploring each of these relationships in the entropy Venn diagram in <a href="#fig-entropy-venn">Figure 6</a>. 🤔 The keen may be curious to learn how Bayes' rule relates
to this equation. This will be the subject of question TBD.</p>
</div>
<div class="paragraph">
<p>From the  <a href="#eqs-mi-from-entropy">MI from Entropy Equations</a> (and entropy Venn diagram) emerge more interesting properties and insights into the interpretation of mutual information through its edge case as shown in <a href="#fig-mi-edge-cases">Figure 7</a>.</p>
</div>
<div id="fig-mi-edge-cases" class="imageblock">
<div class="content">
<img src="images/chapter-03-info-theory/entropy_venn_diagrams_mi_edge_cases.png" alt="Mutual Information Edge Cases" width="900" height="600">
</div>
<div class="title">Figure 7. Mutual Information Edge Cases</div>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Equals Zero</strong>: We previously mentioned that MI is non-negative. In the special case of when \(X\) and \(Y\) are independent \(I(X;Y|X \perp Y)=0\). In the left panel of <a href="#fig-mi-edge-cases">Figure 7</a> this is illustrated by no overlap between the entropy circles. This means: given information about one you learn nothing about the other. This manifests, e.g, in the outcomes of two independent and identically distributed variables, like two dice: by knowing the roll value of one you wouldn&#8217;t gain information about the second.</p>
</li>
<li>
<p><strong>MI is Max Bounded</strong>: Generically speaking \(I(X;Y) \le min[H(X),H(Y)]\), i.e, mutual information is bounded by the smaller entropy. In the Venn diagram in the middle panel of <a href="#fig-mi-edge-cases">Figure 7</a> we see that the intuition of equality is in cases when one entropy fully encapsulates the other. This can manifest in high-cardinality categorical variables. E.g, if \(X\) is ZIP code (e.g., “94114”, “10001”, etc.) and \(Y\) = City (e.g., “San Francisco”, “New York”) we would expect that \(H(Y|X)=0\) (by knowing the ZIP code we immediately know the City, i.e., 0 uncertainty of the City) and therefore \(I(X;Y)=H(Y)\). Conversely, by knowing the City there is decrease in uncertainty about the ZIP code \(H(X|Y)=H(X)-I(X;Y)≥0\) which is zero only in the case where there is only one ZIP code in the City.</p>
</li>
<li>
<p><strong>MI Equals Joint and Marginal Entropies</strong>: In another highly extereme case we may find \(I(X;Y)=H(X)=H(Y)=H(X,Y)\). The Venn diagram in the right panel of <a href="#fig-mi-edge-cases">Figure 7</a> illustrates the case where there is full overlap between \(H(X)\) the \(H(Y)\) entropies. This means that the degree of the uncertainty of both parameters is always the same. This is the most extreme case of max bound shown above. To continue the previous example, imagine a list of very small towns each with only one Zip code, i.e, a 1:1 mapping between Town and Zip code. In this case knowing one directly maps to the other: \(I({\rm ZIP;Town })=H({\rm ZIP})=H({\rm Town})\).</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_mutual_information_is_not_a_metric"><a class="anchor" href="#_mutual_information_is_not_a_metric"></a><a class="link" href="#_mutual_information_is_not_a_metric">3.4.5. Mutual Information is not a Metric</a></h4>
<div class="paragraph">
<p>Recall that in <a href="#section-kld-not-metric">Section 3.3.5</a> we saw that KL divergence is not a metric and as consequence of MI being the KL divergence between the joint probability and the independent joint probability (see the <a href="#eq-mi-kld">Mutual Information as KL Divergence Equation</a>) it follows that MI is not a metric either.</p>
</div>
<div class="paragraph">
<p>A quick refresher of the properties a metric needs to satisfy:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>✅ Non-negativity I(X;Y)≥0</p>
</li>
<li>
<p>✅ Symmetry when exchanging the variables: I(X;Y)=I(Y;X)</p>
</li>
<li>
<p>❌ Identity of indiscernibles: d(X,Y)=0 iff X=Y; We&#8217;ve seen that when X=Y MI is at its maximum and MI is zero only when there is no overlap.</p>
</li>
<li>
<p>❌ Triangle inequality: d(X,Y)≤ d(X,Z) + d(Y,Z).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To examine why the triangle inequality doesn&#8217;t hold for MI let&#8217;s illustrate a simple example of I(X;Y)≰ I(X;Z) + I(Y;Z) in the following entropy Venn diagram now adding a third parameter Z that is independent from X,Y:</p>
</div>
<div id="fig-mi-triangle-violation" class="imageblock text-center">
<div class="content">
<img src="images/chapter-03-info-theory/mi_venn_entropy_triangle_inequality.png" alt="Violating Triangle Inequality Violation" width="600" height="400">
</div>
<div class="title">Figure 8. Mutual Information Triangle Inequality Violation</div>
</div>
<div class="paragraph">
<p>In <a href="#fig-mi-triangle-violation]">[fig-mi-triangle-violation]]</a> wee see that \(I(X;Y)&gt;0\) and \(I(X;Z)=I(Y;Z)=0\) clearly violating the triangle inequality:  \(0&lt;I(X;Y) \lneq 0 + 0\)</p>
</div>
<div class="paragraph">
<p>Alternative metrics have been proposed to address this, most notably the <strong><em>Variation of Information</em></strong> (VI) metric, which is defined as:</p>
</div>
<div id="eq-vi" class="stemblock">
<div class="title">Variation of Information Equation</div>
<div class="content">
\[\begin{align*}
{\rm VI}(X;Y) &amp;= H(X,Y) - I(X;Y)\\ &amp;= H(X|Y) + H(Y|X) \\ &amp;= H(X) + H(Y) - 2I(X;Y)
\end{align*}\]
</div>
</div>
<div class="paragraph">
<p>This metric satisfies all four properties of a metric including the triangle inequality. It may be interpreted as the amount of information that is not shared between two variables. A further derivative called the <strong><em>Rajski distance</em></strong> is defined as \({\rm VI}(X;Y)/H(X,Y)\) to a [0,1] range making it a meaningful bounded distance metric.</p>
</div>
<div class="paragraph">
<p>These are not wildly used in practice, but mentioned here to help us understand more popular variants such as Normalised MI (NMI) and Adjusted MI (AMI) when we discuss their usage in Clustering Performance (section TBD).</p>
</div>
</div>
<div class="sect3">
<h4 id="_summarising_mutual_information"><a class="anchor" href="#_summarising_mutual_information"></a><a class="link" href="#_summarising_mutual_information">3.4.6. Summarising Mutual Information</a></h4>
<div class="paragraph">
<p>In this section we&#8217;ve explored mutual information from two complementary perspectives: as the expected value of point mutual information (PMI) and as a function of entropies. Both views converge to the same understanding of MI as a measure of the shared information between two variables.</p>
</div>
<div class="paragraph">
<p>The PMI perspective provides a granular view, highlighting how individual event pairs contribute to the overall mutual information. This is particularly useful for understanding local dependencies and interactions between specific outcomes.</p>
</div>
<div class="paragraph">
<p>The entropy perspective offers a more holistic view, framing MI in terms of uncertainty reduction. This is especially intuitive when visualised through Venn diagrams, making it easier to grasp the relationships between joint, marginal, and conditional entropies.</p>
</div>
<div class="paragraph">
<p>Both perspectives underscore the versatility and depth of mutual information as a tool for quantifying relationships between variables. Whether you&#8217;re interested in local interactions or global uncertainty, MI provides a robust framework for understanding and measuring dependence.</p>
</div>
<div class="paragraph">
<p>So far we have focused on discrete variables. However, many real-world applications involve continuous variables, such as measurements in physics, finance, or sensor data. Extending information theory concepts to continuous variables introduces new challenges and considerations, which we will explore next.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="section-continuous-info-theory"><a class="anchor" href="#section-continuous-info-theory"></a><a class="link" href="#section-continuous-info-theory">3.5. 🌊 Information Theory for Continuous Variables</a></h3>
<div class="paragraph">
<p>So far we have focused on information theory metrics for categorical variables. But a lot of real-world data is continuous: sensor readings, stock prices, medical measurements, and so forth. Extending information theory into the continuous domain is not trivial. In fact, it is here that even Shannon himself stumbled.</p>
</div>
<div class="paragraph">
<p>In this section we will examine the peculiarities of continuous information. On the one hand, absolute measures of information like entropy cannot be naïvely extended to continuous variables; on the other hand, relative measures like KL divergence and mutual information can. We then explore how to compute KL divergence and mutual information for continuous variables.</p>
</div>
<div class="sect3">
<h4 id="section-differential-entropy"><a class="anchor" href="#section-differential-entropy"></a><a class="link" href="#section-differential-entropy">3.5.1. The Differential Entropy Problem</a></h4>
<div class="paragraph">
<p>In his 1948 paper, Shannon extended his definition of entropy to continuous random variables, which he coined <strong><em>differential entropy</em></strong>. His intuition was to replace the sum with an integral where now
\(X\) is a continuous variable with distribution \(p_X(x)\):</p>
</div>
<div id="eq-differential-entropy" class="stemblock">
<div class="title">Differential Entropy Equation</div>
<div class="content">
\[H(X) = -\int_{-\infty}^{\infty} p_X(x) \log_2(p_X(x)) \, dx\]
</div>
</div>
<div class="paragraph">
<p>At first glance this looks like a seamless extension. Digging a bit deeper, however, one quickly realises that this is flawed.</p>
</div>
<div class="paragraph">
<p>The most important aspect here is that \(p(x)\) here is not a probability but a <strong><em>probability density function</em></strong> (PDF).</p>
</div>
<div class="paragraph">
<p>Recall that this means \(\int_{-\infty}^{\infty} p_X(x) dx = 1\).</p>
</div>
<div class="paragraph">
<p>This has two important implications on the components of differerntial entropy:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Units of measure</strong>: As a PDF \(p_X(x)\) has units of 1/(units of \(X\)). For example, if \(X\) is measured in meters, then \(p(x)\) has units of 1/meters. This creates a problem because the logarithm of a quantity with physical units is not mathematically well-defined.</p>
</li>
<li>
<p><strong>\(p_X(x)\) can be greater than 1</strong>: Unlike probabilities, which are always between 0 and 1, probability densities can exceed 1. For example, a uniform distribution \(p_X(x)=a\) over the interval [0, 0.5] has a density of \(a=2\) in that range (\(1 = \int_0^{0.5}p_X(x) dx= x|_0^{0.5} \cdot  a = 0.5a \rightarrow p_X(x)=a=2\)).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>If one does not realise these flaws several counterintuitive properties of differential entropy emerge:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Can be Negative</strong>: Unlike discrete entropy, which is non-negative, differential entropy can be negative. This occurs when the PDF is highly concentrated in a small region, leading to a high density and thus a negative contribution to the integral. Continuing the example of a uniform distribution over [0, 0.5] has a differential entropy of -1. (\(H(X) = -\int_0^{0.5} 2 \log_2(2) dx = -1\)).</p>
</li>
<li>
<p><strong>Not invariant to transformations</strong>: Differential entropy is not invariant under continuous transformations of the variable. This is unlike discrete entropy, which remains unchanged under re-labeling of categories.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To see the non invariant property, consider definining a new variable \(Y = bX\) for some constant \(b &gt; 0\), the PDF of \(Y\) is given by \(p_Y(y) = \frac{1}{b} p_X\left(\frac{y}{b}\right)\). The differential entropy of \(Y\) can be computed as \(H(Y) = H(X) + \log_2(b)\). This shows that scaling the variable by a factor of \(b\) changes the differential entropy by an additive constant \(\log_2(b)\), demonstrating that differential entropy is varies under scaling transformations.</p>
</div>
<div class="paragraph">
<p>For example, if you scale a variable by a factor of 4, the differential entropy changes by an additive bit (\(\log_2(4)=2\) bits). In practice, this means that the entropy of distance measurments in meters would be \(\log_2(100)\approx 6.6\) bits higher than if measured in centimeters. Alternatively when measuring tempertature in Celzius, Fahrenheit or Kelvin would result in substantially different entropy values due to their linear tranformations.</p>
</div>
<div class="paragraph">
<p>These peculiarities make differential entropy less intuitive and less useful as an absolute measure of uncertainty compared to its discrete counterpart. It&#8217;s unfortunate that in what is considered the <em>Magna Carta of Information Theory</em> Shannon based differential entropy on intuition rather than rigorous derivation, leading to what may be considered his "biggest blunder".</p>
</div>
<div class="paragraph">
<p>The main takeaway is that <strong>absolute</strong> measures of information of continuous variables, like differential entropy, are fraught with issues and should be used with caution.<sup class="footnote">[<a id="_footnoteref_8" class="footnote" href="#_footnotedef_8" title="View footnote.">8</a>]</sup></p>
</div>
<div class="paragraph">
<p>Despite these challenges, <strong>relative</strong> measures of information, such as KL divergence and mutual information, can be extended to continuous variables without the same issues. This is because these measures compare two distributions under the log function so the problematic unit-dependent terms cancel out. In the next sections, we examine how both of these measures retain their interpretive power in the continuous domain.</p>
</div>
</div>
<div class="sect3">
<h4 id="section-continuous-kld"><a class="anchor" href="#section-continuous-kld"></a><a class="link" href="#section-continuous-kld">3.5.2. KL Divergence for Continuous Data</a></h4>
<div class="paragraph">
<p>For a continuous variable \(X\) with PDFs \(p_X(x)\) and \(q_X(x)\), the KL divergence is defined as:</p>
</div>
<div id="eq-continuous-kld" class="stemblock">
<div class="title">Continuous KL Divergence Equation</div>
<div class="content">
\[D_{KL}(p \| q) = \int_{-\infty}^{\infty} p_X(x) \log\left(\frac{p_X(x)}{q_X(x)}\right) \, dx\]
</div>
</div>
<div class="paragraph">
<p>This integral is well-defined as long as \(q_X(x)\) is positive whenever \(p_X(x)\) is non-zero.
KL divergence for continuous variables carries the same interpretation as in the <a href="#section-kl-divergence">discrete case</a>: it quantifies how one distribution diverges from another in units of bits (or nats, depending on the logarithm base).</p>
</div>
<div class="paragraph">
<p>Crucially, unlike <a href="#section-differential-entropy">differential entropy</a>, KL divergence remains well-defined because the density ratio inside the logarithm is dimensionless which also resolves the unit-dependence issue. It is also <strong>invariant to reparameterisations</strong> and preserves the key categorical-case properties: nonnegativity (by Gibbs’ inequality) and asymmetry when the distributions are swapped.</p>
</div>
<div class="paragraph">
<p>In data science, KL divergence with continuous data arises in <strong>model monitoring</strong>. For example, if we fit a Gaussian distribution to a feature during training, and later compute the KL divergence between the observed feature distribution and the training baseline. A sharp increase may signal dataset shift and the need for retraining.</p>
</div>
<div class="paragraph">
<p>In machine learning KL divergence is central to <strong>variational inference</strong> methods such as variational autoencoders, where complex posteriors are approximated by families through minimization of the KL divergence.</p>
</div>
<div class="sect4">
<h5 id="section-continuous-kld-intuition"><a class="anchor" href="#section-continuous-kld-intuition"></a><a class="link" href="#section-continuous-kld-intuition">Continuous KL Divergence Intuition</a></h5>
<div class="paragraph">
<p>As discussed in the <a href="#section-kl-divergence">discrete case</a>, KL divergence interpretation varies by context. Here we briefly examine a few distributions to get an intuition for meaning of information loss in terms of bits from Gaussian distributions.</p>
</div>
<div class="paragraph">
<p>In the following script we generate four pairs of Gaussian PDFs and compute the KL divergence between them. We then plot the distributions and annotate the KL divergence in bits.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm
FIG_WIDTH, FIG_HEIGHT = 8, 6
def plot_distributions_and_kld(attr_p, attr_q):
    # Define two Gaussian distributions
    p = norm(**attr_p)   # e.g, N(0,1)
    q = norm(**attr_q)   # e.g, N(1,1.5)

    # Approximate KL divergence using numerical integration
    xs = np.linspace(-10, 10, 1000)
    px = p.pdf(xs)
    qx = q.pdf(xs)

    kl = np.sum(px * np.log(px / qx)) * (xs[1] - xs[0]) / np.log(2)  # in bits

    plt.plot(xs, px, label=f'p(x) ~ N({attr_p["loc"]},{attr_p["scale"]})', color="purple")
    plt.plot(xs, qx, label=f'q(x) ~ N({attr_q["loc"]},{attr_q["scale"]})', color="orange")
    # plt.yscale('log')  # toggle to see the tails better
    plt.title(f"KL(p||q) ≈ {kl:.3f} bits")
    plt.grid(alpha=0.3)
    plt.xlabel(r'$x$')
    plt.legend()


plt.figure(figsize=(FIG_WIDTH * 3, 2 * FIG_HEIGHT))

plt.subplot(2, 2, 1)
plot_distributions_and_kld({"loc": 0, "scale": 1}, {"loc": 0.2, "scale": 1.})
plt.subplot(2, 2, 2)
plot_distributions_and_kld({"loc": 0, "scale": 1}, {"loc": 0, "scale": 2.07})
plt.subplot(2, 2, 3)
plot_distributions_and_kld({"loc": 0, "scale": 1}, {"loc": 1.18, "scale": 1.})
plt.subplot(2, 2, 4)
plot_distributions_and_kld({"loc": 0, "scale": 1}, {"loc": 1.74, "scale": 2.})
plt.tight_layout()</code></pre>
</div>
</div>
<div id="fig-kld-gaussians" class="imageblock text-center">
<div class="content">
<img src="images/chapter-03-info-theory/kld_continous_4gaussian_pairs.png" alt="KL Divergence Between Gaussian Distributions">
</div>
<div class="title">Figure 9. KL Divergence Between Gaussian Distributions</div>
</div>
<div class="paragraph">
<p>In the top left panel of <a href="#fig-kld-gaussians">Figure 9</a> we see two Gaussians with the same variance but slightly different means (0 and 0.2). The KL divergence is small (0.03 bits) showing that the two distributions are nearly indistinguishable. This means the approximating \(p_X(x)\) with \(q_X(x)\) leads to negligible information loss.</p>
</div>
<div class="paragraph">
<p>In the top right panel we see two Gaussians with the same mean (0) but different variances (1 and 2.07). The KL divergence is higher (0.5 bits), reflecting the mismatch in spread. Using \(q_X(x)\) discards more information than in the first case.</p>
</div>
<div class="paragraph">
<p>In the bottom panels both cases yield about 1 bit of divergence, but for different reasons. In the left panel, the means are farther apart (0 vs 1.18). In the right panel, both means and variances differ (0 vs 1.74 and 1 vs 2). Despite these differences, both scenarios lead to the same KL value of 1 bit: a substantial information loss when substituting \(q_X(x)\) for latexmath:[p_X(x).</p>
</div>
<div class="paragraph">
<p>In the bottom left and right panels results in the same 1 bit difference but in different ways: in the left panel the means are further apart (0 and 1.18) while in the right panel both the means and variances differ (0 vs 1.74 and 1 vs 2). Both scenarios lead to a KL divergence of 1 bit, indicating that using \(q_X(x)\) would result in a substantial information loss.</p>
</div>
</div>
<div class="sect4">
<h5 id="section-continuous-kld-practice"><a class="anchor" href="#section-continuous-kld-practice"></a><a class="link" href="#section-continuous-kld-practice">Continuous KL Divergence In Practice</a></h5>
<div class="paragraph">
<p>In practice, computing KL divergence for continuous variables involves two main challenges:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Estimating the underlying PDFs \(p_X(x)\) and \(q_X(x)\) from finite samples.</p>
</li>
<li>
<p>Dealing with cases where the distributions do not fully overlap.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Unlike in the discrete case, where probabilities can be estimated directly from counts, for continuous variables the true PDFs are rarely known.<sup class="footnote">[<a id="_footnoteref_9" class="footnote" href="#_footnotedef_9" title="View footnote.">9</a>]</sup> Common estimation strategies include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Parametric Methods</strong>: Assume a specific distributional form (e.g., Gaussian, Exponential) and estimate parameters using Maximum Likelihood Estimation (MLE).</p>
</li>
<li>
<p><strong>Non-Parametric Methods</strong>: Use flexible techniques such as kernel density estimation (KDE) or histograms to approximate the PDF without assuming a fixed form.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Even once PDFs are estimated, difficulties remain when the distributions do not overlap. The Continuous KL Divergence Equation requires \(q_X(x) &gt; 0\) wherever \(p_X(x) &gt; 0\). If \(q_X(x)=0\) where \(p_X(x)\) has mass, KL divergence is undefined and tends to infinity.</p>
</div>
<div class="paragraph">
<p>Practical approaches depend on the degree of overlap between \(p_X(x)\) and \(q_X(x)\):</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Complete Overlap: The <a href="#eq-continuous-kld">Continuous KL Divergence Equation</a> can be used directly.</p>
</li>
<li>
<p>Some missing Overlap: Apply smoothing (e.g., KDE or adding a small \(\epsilon\)) so both densities have nonzero support everywhere.</p>
</li>
<li>
<p>No Overlap: KL divergence becomes infinite. In such cases, alternatives like the <a href="#section-kld-not-metric">Jensen-Shannon divergence</a> are preferred, since it is symmetric and always finite.</p>
</li>
</ul>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
While Jensen-Shannon divergence is symmetric and always finite, it can mask sharp differences between distributions that KL divergence highlights. In practice, JS is useful when comparing empirical distributions with limited overlap, but if you need sensitivity to rare events or tail behavior, KL remains the more informative choice.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>For examples and further details see the <em>KL Divergence for Continuous Variables</em> notebook (link TBD).</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="section-continuous-mi"><a class="anchor" href="#section-continuous-mi"></a><a class="link" href="#section-continuous-mi">3.5.3. Mutual Information for Continuous Data</a></h4>
<div class="paragraph">
<p>Mutual information for continuous variables is defined similarly to the discrete case, but with integrals replacing sums. For two continuous random variables \(X\) and \(Y\) with joint PDF \(p_{X,Y}(x,y)\) and marginal PDFs \(p_X(x)\) and \(p_Y(y)\), the mutual information is given by:</p>
</div>
<div id="eq-continuous-mi" class="stemblock">
<div class="title">Continuous Mutual Information Equation</div>
<div class="content">
\[I(X;Y) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} p_{X,Y}(x,y) \log\left(\frac{p_{X,Y}(x,y)}{p_X(x)p_Y(y)}\right) \, dx \, dy\]
</div>
</div>
<div class="paragraph">
<p>This integral quantifies the amount of information that \(X\) and \(Y\) share.
It retains the same properties as in the discrete case, such as non-negativity and symmetry under exchanging the roles of \(X\) and \(Y\). Because it is defined as a ratio of densities, the logarithm is dimensionless, avoiding the unit-dependence issue that affects <a href="#section-differential-entropy">differential entropy</a>.</p>
</div>
<div class="sect4">
<h5 id="section-continuous-mi-intuition"><a class="anchor" href="#section-continuous-mi-intuition"></a><a class="link" href="#section-continuous-mi-intuition">Continuous MI Intuition</a></h5>
<div class="paragraph">
<p>The interpretation of mutual information for continuous variables parallels the discrete case: it measures how much knowing one variable reduces uncertainty regarding the other. For example, in machine learning it is widely used for feature selection, where we seek to identify features that share the most information with the target variable and thus help reduce uncertainty when predicting it. In neuroscience, mutual information is often applied to quantify how much a neural response tells us about an external stimulus, providing a principled measure of coding efficiency in sensory systems.</p>
</div>
<div class="paragraph">
<p>To gain an intuition in the continuous variable setting, let&#8217;s examine a specific example. Consider two continuous variables \(X\) and \(Y\) that are jointly Gaussian with a certain correlation coefficient \(\rho_{X,Y}\). The mutual information between them can be computed as:<sup class="footnote">[<a id="_footnoteref_10" class="footnote" href="#_footnotedef_10" title="View footnote.">10</a>]</sup></p>
</div>
<div id="eq-mi-jointly-gaussian" class="stemblock">
<div class="title">MI for Jointly Gaussian Variables Equation</div>
<div class="content">
\[I(X;Y) = -\frac{1}{2} \log_2(1 - \rho_{X,Y}^2)\]
</div>
</div>
<div class="paragraph">
<p>This equation shows that as the correlation \(\rho_{X,Y}\) approaches 1 or -1 (perfect correlation), the mutual information increases, indicating that knowing one variable provides significant information about the other. Conversely, when \(\rho_{X,Y}\) is 0 (independence), the mutual information is zero, meaning knowing one variable does not reduce uncertainty about the other. The dependence on \(\rho_{X,Y}^2\) shows that mutual information is unaffected by the sign of the correlation.</p>
</div>
<div class="paragraph">
<p>Note that the mutual information for continuous variables can be infinite in certain cases, such as when one variable is a deterministic function of the other (e.g, in the linear case where \(\rho = \pm 1\)). This is a key difference from the discrete case, where mutual information is always finite and bounded by the smaller entropy of the two variables (as seen in <a href="#fig-mi-edge-cases">Figure 7</a>).</p>
</div>
<div class="paragraph">
<p>Let&#8217;s visualise this with a simple Python script that computes and plots the mutual information for jointly Gaussian variables with varying correlation coefficients.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal
FIG_WIDTH, FIG_HEIGHT = 8, 6
def mutual_information_gaussian(rho):
    return -0.5 * np.log2(1 - rho**2)

rhos = np.linspace(-0.99, 0.99, 100)
mis = [mutual_information_gaussian(rho) for rho in rhos]

plt.figure(figsize=(FIG_WIDTH, FIG_HEIGHT))
plt.plot(rhos, mis, label='Mutual Information')
ho_1bit = np.sqrt(3/4)
plt.plot([rho_1bit, rho_1bit], [0, mutual_information_gaussian(rho_1bit)], color="gray", lw=1, ls="--")  # Mark the point for 1 bit
plt.plot([-rho_1bit, -rho_1bit], [0, mutual_information_gaussian(rho_1bit)], color="gray", lw=1, ls="--")  # Mark the point for 1 bit
plt.title('Mutual Information for Jointly Gaussian Variables')
plt.xlabel('Correlation Coefficient (rho)')
plt.ylabel('Mutual Information (bits)')
plt.axhline(0, color='black', lw=0.5, ls='--')
plt.axvline(0, color='black', lw=0.5, ls='--')
plt.legend()
plt.grid()
plt.show()</code></pre>
</div>
</div>
<div id="fig-mi-gaussian-correlation" class="imageblock text-center">
<div class="content">
<img src="images/chapter-03-info-theory/bivariate_gaussian_rho_vs_mi.png" alt="MI for Jointly Gaussian Variables vs Correlation">
</div>
<div class="title">Figure 10. MI for Jointly Gaussian Variables vs Correlation</div>
</div>
<div class="paragraph">
<p>In <a href="#fig-mi-gaussian-correlation">Figure 10</a> we see that as the correlation coefficient \(\rho\) approaches ±1, the mutual information increases sharply, indicating a strong dependency between the variables. When \(\rho\) is 0, the mutual information is 0, reflecting independence. We&#8217;ve also highlighted that mutual information reaches 1 bit at \(\rho \approx \pm 0.866\) (\(\sqrt{3/4}\)).</p>
</div>
<div class="paragraph">
<p>It&#8217;s also instructive to visualize the joint distributions for different correlation coefficients.  <a href="#fig-sixs-joint-gaussians">Figure 11</a> shows six bivariate Gaussians \(\rho = 0.05, 0.12, 0.3, 0.5, 0.75, -0.9\). For each it displays their 1-3 standard deviation contours of the joint probabilities \(p_{X,Y}(x,y)\) in color and the \(3\sigma\) contour of the corresponding independent joint probabilities \(p_X(x)p_Y(y)\) in light gray.</p>
</div>
<div id="fig-sixs-joint-gaussians" class="imageblock text-center">
<div class="content">
<img src="images/chapter-03-info-theory/six_bivariate_gaussian_jointprobs.png" alt="MI for Jointly Gaussian Variables vs Correlation">
</div>
<div class="title">Figure 11. Bivariate for Jointly Gaussian Variables vs Correlation</div>
</div>
<div class="paragraph">
<p>When contrasting by eye the joint distributions \(p_{X,Y}(x,y)\) (in color) with the independent joint distributions \(p_X(x)p_Y(y)\) (in light gray) we can see how the the former would be "surprising" when compared to the latter. This is the essence of mutual information: on average, how much more surprising the true joint distribution is relative to the assumption of independence.</p>
</div>
<div class="paragraph">
<p>As the correlation increases, the joint distribution becomes more elongated along the diagonal, indicating a stronger relationship between the variables and higher mutual information. Also note that mutual information is always nonnegative, even when the correlation is negative.</p>
</div>
<div class="paragraph">
<p>The <a href="#eq-mi-jointly-gaussian">closed-form expression for MI in jointly Gaussian variables</a> is a special case that provides a clear link between correlation and mutual information. However, mutual information captures dependencies beyond linear correlations. To illustrate this, <a href="#fig-three-shapes-pearson-mi">Figure 12</a> considers three different relationships between two continuous variables: linear, parabolic, and circular. For this purpose we generate synthetic datasets for each relationship and calculate their Pearson correlation coefficient and mutual information.</p>
</div>
<div id="fig-three-shapes-pearson-mi" class="imageblock text-center">
<div class="content">
<img src="images/chapter-03-info-theory/three_shapes_pearson_mi.png" alt="MI vs. Correlation for Linear" width="Parabolic and Circular Relationships">
</div>
<div class="title">Figure 12. MI vs. Correlation for Linear, Parabolic and Circular Relationships</div>
</div>
<div class="paragraph">
<p>By design all three panels contain distributions with a mutual information of approximately 1 bit, which means that knowing one variable reduces the uncertainty about the other by half. However, their Pearson correlation coefficients differ significantly: the linear relationship has a high correlation (0.94) whereas the parabolic and circular relationships have near-zero correlation.</p>
</div>
<div class="paragraph">
<p>This illustrates a key advantage of mutual information: it captures all types of dependencies, not just linear ones.</p>
</div>
<div class="paragraph">
<p>Note that here we did not use the <a href="#eq-continuous-mi">Continuous Mutual Information Equation</a> directly to compute the mutual information. The reason is in general, computing mutual information from continuous distributions is more difficult, since it requires estimating both the joint and marginal PDFs from data. This is especially true for arbitrary continuous distributions. Instead, we used the sklearn <code>mutual_info_regression</code> to estimate the mutual information from the samples. This function uses a k-nearest neighbors approach to estimate MI, which is effective for continuous variables. Next we discuss in more detail the practical challenges of estimating MI for continuous variables.</p>
</div>
<div class="paragraph">
<p>In the examples in <a href="#fig-three-shapes-pearson-mi">Figure 12</a> we did not compute mutual information from the <a href="#eq-continuous-mi">closed-form equation</a> but estimated it directly from data. This reflects the more general situation: for arbitrary continuous variables, calculating MI requires estimating both the joint and marginal PDFs, which is rarely straightforward. Instead, we relied on sklearn&#8217;s <code>mutual_info_regression</code>, which implements a k-nearest neighbors estimator well suited to continuous samples. The need for such estimators highlights the practical challenges of working with continuous MI, which we turn to next.</p>
</div>
</div>
<div class="sect4">
<h5 id="section-continuous-mi-practice"><a class="anchor" href="#section-continuous-mi-practice"></a><a class="link" href="#section-continuous-mi-practice">Continuous MI In Practice</a></h5>
<div class="paragraph">
<p>In <a href="#section-continuous-kld-practice">Section 3.5.2.2</a> we discussed the challenges of estimating KL divergence for continuous variables. Considering <a href="#eq-mi-kld">mutual information is a KL divergence</a> it is reasonable to assume that continuous MI faces similar issues.This is true for the density estimation problem but not for the support overlap problem.</p>
</div>
<div class="paragraph">
<p>The density estimation problem for MI is more difficult than KL divergence, because it involves joint estimation of multiple variables rather than a single density. Common approaches include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Binning</strong>: Discretising the continuous variables into bins and then applying the discrete MI formula. The choice of bin size can significantly affect the estimate.</p>
</li>
<li>
<p><strong>Kernel Density Estimation (KDE)</strong>: Estimating the joint and marginal PDFs using KDE and then applying the continuous MI formula.</p>
</li>
<li>
<p><strong>K-Nearest Neighbors (KNN)</strong>: Employing KNN-based estimators that estimate MI directly from samples without explicitly computing the PDFs.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The support mismatch problem that arises in KL divergence is not an issue for MI. This is because the marginals \(p_X(x)\) and \(p_Y(y)\) are derived from the same joint distribution \(p_{X,Y}(x,y)\), ensuring they share the same support. In other words, whenever \(p_{X,Y}(x_i,y_i)&gt;0\) the corresponding  \(p_{X}(x_i)p_{Y}(x_j)\) will also be positive, so the integral remains well-defined.</p>
</div>
<div class="paragraph">
<p>One last issue that is specific to mutual information that may arise is handling heterogeneous variable types. Real datasets may contain a mix of continuous and categorical variables. Common approaches include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Discretisation</strong>: Converting continuous variables into categorical by binning, then applying the discrete MI formula.</p>
</li>
<li>
<p><strong>Hybrid Estimators</strong>: Using estimators designed for mixed data types, such as copula-based methods.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This makes mutual information a versatile tool for measuring dependencies in datasets with diverse variable types, supporting applications such as feature selection, clustering, and general dependency analysis.</p>
</div>
<div class="paragraph">
<p>For examples and further details see the <em>Mutual Information for Continuous Variables</em> notebook (link TBD).</p>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="section-info-theory-summary"><a class="anchor" href="#section-info-theory-summary"></a><a class="link" href="#section-info-theory-summary">3.6. Information Theory Summary</a></h3>
<div class="paragraph">
<p>In this chapter we explored core ideas of information theory, focusing on entropy, KL divergence, and mutual information. We began with discrete variables, establishing the definitions, properties, and interpretations of these key metrics and their foundations. We then extended our discussion to continuous variables, highlighting the challenges and nuances that arise in this domain.</p>
</div>
<div class="paragraph">
<p>Key takeaways include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Entropy</strong> quantifies uncertainty in a random variable, with higher entropy indicating greater unpredictability. For discrete variables it has a clean interpretation, while for continuous variables (differential entropy) it must be treated with caution because of unit-dependence.</p>
</li>
<li>
<p><strong>KL Divergence</strong> measures how one probability distribution diverges from another. Unlike entropy, KL remains well-defined in the continuous case because the density ratio is dimensionless</p>
</li>
<li>
<p><strong>Mutual Information</strong> captures the amount of information that one variable provides about another. It generalises cleanly to the continuous case and is particularly useful for detecting nonlinear dependencies.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The concepts introduced here form the theoretical basis for many advanced methods in machine learning, statistics, and data science that we will revisit throughout the book.</p>
</div>
<div class="paragraph">
<p>In later chapters we will broaden this toolkit, examining different forms of uncertainty (aleatoric vs epistemic), alternative divergence measures beyond KL, and additional ways of quantifying dependency beyond mutual information.</p>
</div>
<div class="paragraph">
<p><strong>🤔 Questions for Review</strong></p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>What are the key axioms upon which Shannon built his definition of self-information?</p>
</li>
<li>
<p>What is the entropy of a routllette wheel with 38 slots (1-36, 0, 00)? What is the entropy of a fair die with 6 sides? Which has higher entropy and why?</p>
</li>
<li>
<p>What is the relationship between KL divergence and cross-entropy?</p>
</li>
<li>
<p>On the one hand we discussed the symmetry of MI and on the other hand the assymetry of KL divergence. Considering that MI is a KL divergence, how can both statements be true?</p>
</li>
<li>
<p>In the case of two categorical variables, what is the maximum possible value of mutual information? When is this achieved? What about for two continuous variables?</p>
</li>
<li>
<p>Given a pair of fair dice:</p>
<div class="ulist">
<ul>
<li>
<p>a) What is the point mutual information of the outcome of one die given the outcome of the other?</p>
</li>
<li>
<p>b) What is the mutual information between the two dice?</p>
</li>
<li>
<p>c) How do your answers for (a) and (b) change for an \(N\)-sided die?</p>
</li>
<li>
<p>d) How would your answers for (a), (b) and (c) change if the two dice were biased such that the outcome of one die is always equal to the outcome of the other? (Imagine some weird quantum entanglement between the two dice.)</p>
</li>
</ul>
</div>
</li>
<li>
<p>Recall the Monty Hall problem from <a href="#chapter-combinatorics-probabilities">Chapter 2</a>:</p>
<div class="ulist">
<ul>
<li>
<p>a) What is the entropy of the initial choice of door? (Recall that all doors are equally likely.)</p>
</li>
<li>
<p>b) What is the entropy of the choice of door after Monty reveals a goat behind one of the unchosen doors? (Hint: It is useful to think in terms of \(p_{\rm A}=p_{\rm B}=\frac{1}{2}\) and \(p_{\rm C}=0\) where \(\rm C\) is the door Monty opened.)</p>
</li>
<li>
<p>c) How would you use KL divergence to quantify the information gained by switching doors after Monty reveals a goat behind one of the unchosen doors?</p>
</li>
<li>
<p>d) Extend you answer in part (c) where now there are \(N&gt;4\) doors, one with a car and \(N-1\) with goats. Monty opens \(N-2\) doors revealing goats. What is the KL divergence between the initial and updated distributions over doors?</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_interpreting_associations"><a class="anchor" href="#_interpreting_associations"></a><a class="link" href="#_interpreting_associations">4. Interpreting Associations</a></h2>
<div class="sectionbody">

</div>
</div>
<h1 id="_part_ii_reasoning_under_uncertainty" class="sect0"><a class="anchor" href="#_part_ii_reasoning_under_uncertainty"></a><a class="link" href="#_part_ii_reasoning_under_uncertainty">Part II Reasoning Under Uncertainty</a></h1>
<div class="sect1">
<h2 id="_understanding_and_managing_uncertainty"><a class="anchor" href="#_understanding_and_managing_uncertainty"></a><a class="link" href="#_understanding_and_managing_uncertainty">5. Understanding and Managing Uncertainty</a></h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="uncertainty-types"><a class="anchor" href="#uncertainty-types"></a><a class="link" href="#uncertainty-types">5.1. Types of Uncertainty</a></h3>

</div>
</div>
</div>
<div class="sect1">
<h2 id="_appendix_a_additional_resources"><a class="anchor" href="#_appendix_a_additional_resources"></a><a class="link" href="#_appendix_a_additional_resources">Appendix A: Appendix A: Additional Resources</a></h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_bibliography"><a class="anchor" href="#_bibliography"></a><a class="link" href="#_bibliography">Bibliography</a></h2>
<div class="sectionbody">
<div class="ulist bibliography">
<ul class="bibliography">
<li>
<p><a id="shannon1948"></a>[shannon1948] Shannon, C. E. (1948). "A Mathematical Theory of Communication." <em>The Bell System Technical Journal</em>, 27(3), 379-423.</p>
</li>
<li>
<p><a id="jurafsky_martin2021"></a>[jurafsky_martin2021] Jurafsky, D., &amp; Martin, J. H. (2021). <em>Speech and Language Processing</em>. 3rd ed. Prentice Hall.</p>
</li>
<li>
<p><a id="goodfellow2014"></a>[goodfellow2014] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., &#8230;&#8203; &amp; Bengio, Y. (2014). "Generative Adversarial Nets." <em>Advances in Neural Information Processing Systems</em>, 27.</p>
</li>
<li>
<p><a id="ref1"></a>[ref1] Author, A. (2024). <em>Book Title</em>. Publisher.</p>
</li>
<li>
<p><a id="ref2"></a>[ref2] Smith, J. (2023). "Article Title." <em>Journal Name</em>, 15(3), 123-145.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_index"><a class="anchor" href="#_index"></a><a class="link" href="#_index">Index</a></h2>
<div class="sectionbody">

</div>
</div>
</div>
<div id="footnotes">
<hr>
<div class="footnote" id="_footnotedef_1">
<a href="#_footnoteref_1">1</a>. Bruno Munari (1907-1998) was an artist, designer and inventor renowned for blending scientific logic, mischievous play, and visual wit, traits that mirrored Claude Shannon&#8217;s (1916-2001) own approach to both science and life. Curta hand-held calculators were a marvel of engineering in Shannon&#8217;s time, combining precision mechanics with a compact design, making them a favorite among mathematicians and engineers.
</div>
<div class="footnote" id="_footnotedef_2">
<a href="#_footnoteref_2">2</a>. Uncertainty in the context of information theory is aleatoric (irreducible variability), as opposed to epistemic uncertainty (knowledge gaps) as we will further discuss in <a href="#uncertainty-types">Section 5.1</a>.
</div>
<div class="footnote" id="_footnotedef_3">
<a href="#_footnoteref_3">3</a>. You might be familiar with the term entropy from physics. Is there a connection? Shannon borrowed the term from statistical mechanics, where entropy is defined with a formula structurally similar to his: both involve summing terms weighted by probability in the form \(p \log(p)\). A full discussion lies beyond the scope of this book, but while the mathematical parallel is elegant, the physical and informational notions of entropy serve different purposes and are not directly interchangeable in practice.
</div>
<div class="footnote" id="_footnotedef_4">
<a href="#_footnoteref_4">4</a>. In some circles where entropy is applied as a diversity index it is called <em>Shannon diversity index</em> or the <em>Shannon-Wiener index</em>.
</div>
<div class="footnote" id="_footnotedef_5">
<a href="#_footnoteref_5">5</a>. To fully appreciate the concept of cross-entropy penalising incorrect encoding requires an understanding of basics of data compresssion. This is beyond the scope of this book. For those interested in this fascinating topic, however, I recommend going over the complementary <em>Message Length Optimisation</em> notebook (link TBD).
</div>
<div class="footnote" id="_footnotedef_6">
<a href="#_footnoteref_6">6</a>. This is known as the Neyman-Pearson lemma.
</div>
<div class="footnote" id="_footnotedef_7">
<a href="#_footnoteref_7">7</a>. Continuous KL divergence is covered in <a href="#section-continuous-kld">Section 3.5.2</a>.
</div>
<div class="footnote" id="_footnotedef_8">
<a href="#_footnoteref_8">8</a>. TK mention that is is used and reffer to Wiki Page.
</div>
<div class="footnote" id="_footnotedef_9">
<a href="#_footnoteref_9">9</a>. Note that also in the categorical case the actual counts may be a sample that is not representative of the true distribution. We discuss this more in the section of aleatoric and epistemic uncertainty. (Section TBD)
</div>
<div class="footnote" id="_footnotedef_10">
<a href="#_footnoteref_10">10</a>. The full derivation is based on TBD
</div>
</div>
<div id="footer">
<div id="footer-text">
Version 1.0<br>
Last updated 2025-09-30 07:53:39 +0100
</div>
</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.3/highlight.min.js"></script>
<script>
if (!hljs.initHighlighting.called) {
  hljs.initHighlighting.called = true
  ;[].slice.call(document.querySelectorAll('pre.highlight > code[data-lang]')).forEach(function (el) { hljs.highlightBlock(el) })
}
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  messageStyle: "none",
  tex2jax: {
    inlineMath: [["\\(", "\\)"]],
    displayMath: [["\\[", "\\]"]],
    ignoreClass: "nostem|nolatexmath"
  },
  asciimath2jax: {
    delimiters: [["\\$", "\\$"]],
    ignoreClass: "nostem|noasciimath"
  },
  TeX: { equationNumbers: { autoNumber: "none" } }
})
MathJax.Hub.Register.StartupHook("AsciiMath Jax Ready", function () {
  MathJax.InputJax.AsciiMath.postfilterHooks.Add(function (data, node) {
    if ((node = data.script.parentNode) && (node = node.parentNode) && node.classList.contains("stemblock")) {
      data.math.root.display = "block"
    }
    return data
  })
})
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>
</body>
</html>