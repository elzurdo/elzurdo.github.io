---
layout: post
title: "No Gut (Intuition) No Glory: My First A/B Test"
--- 

One of the holy grails of Data Scientists is to see their predictive models have positive impact on the world. 
The impact doesn't have to be big or meaningful for humanity, but rather and intellectual indication that 
that what they see on their computer screen is has a touch with reality.
Unfortunately, however, a lot projects end with a binary result: The stake holder either likes it or doesn't. 

As a Data Scientist in my second year got had the pleasure of seeing one of my projects tested in the field and I got quantitive feedback in which I could measure the impact. The purpose of this post is my self-documentation of what I have done and accomplished. 


# STAR style
My STAR-ish repsonse in interviews about this Case Study

**Situation**  I was assigned the first commercial client of our company. (CA were transitioning from political to commercial.) The client was a midsize restaurant in the USA with over 40 branches who were looking into having my company replace their current marketing vendor. I was the only data scientist on the project along with a project manager. They were conducting a promotion: physical coupons were sent to houses to be redeemed at store locations.


**Task** For 7 branches across 3 markets, I was assigned to generate lists of to drive-in new customers that would redeem more coupons than a list generated by their current vendor.


**Action** Using their data I designed and implemented an A/B test. Created predictive models for each branch. 
One main assumption to be made was the number of households to send in order to get statistically significant results, which is a function both of redemption rates, and even more critically maximum distance from stores. Whereas their vendor was focused on 1-2 miles based on their data and personal experience in the USA I decided to go out to a distance of 6-8 miles depending on the branch.


**Result** In the report I presented to the client I have shown that:  
* My models overall brought in more walk-ins: 8% uplift (for every 100 of theirs I brought in 108) where highlighted branches: 48% and 17% uplift.
* The client learned that they can find new customers at much larger distances than 1-2 miles.
* I also pointed out that the age should not be limited as the other vendor dictated (they limited to educated people up to age 60, whereas I saw that I could bring in 80 year olds)
* My company worked only on per individual models (consumers, voters), but I detected a strong bias in the clien'ts data which meant I had to model by household. To this end I created pipelines (SQL tables and API calls) to model for this aggregated result.  


# In Detail
Here I am answering questions regarding this project which I mostly answered for a screening phase in a Google interview.  

**Describe an interesting applied statistics problem that you have worked on.** 

(Points A-D are addressed below. Here I explain the assignment motivation, planning and results.)

I planned and executed an A/B test to demonstrate to a client that my model can improve their customer targeting efforts compared to their regular vendor.

The client, a mid-size restaurant chain in the USA, launched a promotion with the objective of attracting new customers to seven branches across three states. They mailed physical coupon postcards to households where the prospective customers were required to redeem in the stores (not online).

The client has a standard vendor that creates household mailing lists, prints the postcards and conducts mailing, collection and analysis. Representing a competing marketing firm, my objective was to create a mailing list that would result in more in-store redemptions. 

My responsibilities included:
Designing the test, e.g, predicting the number of households required for statistically significant uplift measurement while keeping costs low.
Creating an alternative household mailing list based on data driven models
Report results

Test result:
My list outperformed that of the vendor on various levels and shown to be statistically significant. The overall uplift was 8% (i.e, the competitors’ households redeemed at a rate of 8% and my households by 8.7%). For one branch in particular my model demonstrated a 48% uplift (this result is shown in the figure below). Also, two of the three states were considered “new markets” for which I achieved an uplift of 17%.


The following figure demonstrates both the final results and the data used to decide the maximum radius of the A/B test for one of the seven branches. Redemption rates are shown as a function of distance, where mine is in red, the vendor’s in black. The shaded regions indicate the 95% CL regions. The light blue dashed line shows the ratio of the number of online customer households (used as training data) to the general public (normalised for convenience). The number of houses mailed per radius bin by the vendor and my company (Cambridge Analytica; CA) are indicated (total of 3,920 households each). 

![image]({{ site.url }}/assets/a-b-test_restaurant.png)  
Whereas the competitor focused on targeting prospective customers around the branch (out to two miles; black), my data driven assumption of extending out to six miles appeared to be successful (red). An interesting result is that between three miles and six the redemption rate was constant. The six mile radius was determined by the distribution of online customers (dashed blue line), for which we see a drop off beyond six miles.


**A. How big was the data set (how many parameters/fields the dataset contained)?**  
For the purpose of these models I used 700 features per sample. The features were based on demographic, education/income, location, commercial and luxury data. The sample size for modeling was on the order of thousands to tens of thousands (depending on the branch/state) and extrapolation 700,000. The final mailing list (for each company) was 25,000 households.

**B. What practical concerns were there for dealing with the data: did you know how it was collected?, were there dirty data problems?, was there sampling bias?, etc.**

I used a few sources of data which had different biases and cleaning requirements:

Defining a negative training set for a binary classification (customers are the positive set).
Ideally for the negative set one would like non-restaurant goers, and/or people who do not like the brand. Considering that I did not have this information and I extrapolated on the general public, I used a sample of the general population as the negative set.
So I was modeling a classification between  “known customers” and “likely not customers”.
Online customer data (positive training set) - biased towards tech savvy.
Past promotion data (positive and negative training sets) - in-store customers but biassed in selection (by vendor): Age, education, income, distance from branches (limited to 2 miles). The advantage of this data was to compare redeemers vs. non-redeemers.
Internal databases of individuals (features)- My company has two semi-overlapping databases. One is based on the USA public voter registry and the second from credit card owners (e.g from Experian). I used both of these to reach as many relevant houses as possible.
This was my company’s first commercial (non political) project, so I was pioneering the use of the credit card owner database. 
Competitor’s list: Overlap and Known customers - I verified that the mailing lists of mine and the vendor’s were unique and tagged known customers to exclude them from the A/B test (the objective was to focus on new customers). This proved to be crucial because the known clients redeemed at 21% whereas the “likely new customers” redeemed at 8.3%.
Redemption rate predictions - the previous rates were based on old and new customers. I had to predict based on new.
Gender bias (positive training sets)- In the training sets, the online customers were mostly female (probably more likely to take initiative in ordering at home or at work), and redemption data predominantly male (vendor selection bias; They chose male names in each household). I concluded by creating household models rather than on an individual level.
Branch opening date and popularity (training sets)- The popularity of the seven branches differed due to both location and opening dates. A later opening date meant less training data. This resulted in me creating models for populations around each branch individually, as well as models for populations around multiple branches.

**C. How did you choose the methodology for the problem? (i.e was it industry standard?, was it tailored for this data?, etc.)**  
For all stages of the A/B test (planning, modelling, reporting) I created algorithms based on python packages (scipy, scikit learn, seaborn and our internal github repository to which I contribute). For statistics I used both Classical and Bayesian inference and for modeling I chose Gradient Boosting Trees (for details see response to the next question).  
I also explored the training set, model predictions and results on maps for which I used the python folium and gmaps packages (the latter renders Google Maps into Jupyter).

**D. How did you implement the methodology?** 

Test Design - I used Bayesian inference techniques to predict the number of pieces of mail that would be required to obtain significant statistics to show an uplift. In particular I used Beta distribution sampling (because the households could either redeem or not, like a flip of a coin) with various data driven assumptions: redemption rates, expected uplift. 
A crucial consideration was the maximum radius from the branches chosen for targeting. Although the vendor decided to mail out to two miles, the online customer data gave me the confidence to target up to six miles. The data used for this decision and the result is demonstrated in the figure above.
Finally, to manage result expectations, I quickly realised that it would be highly costly to send out sufficient mail to resolve significant uplift measurements for each of the seven branches (given the assumed redemption rates and uplifts). We agreed that the test would be comparing the overall redemption rates across all branches collectively.

Modeling - I explored both Gradient Boosting Trees classifier and K-Means clustering because of the  non-linear interactions between features and target variables. I chose GBT because of advantages of usage of more features as well as useful tools to determine probabilities and feature importances. 
As this is a geo-sensitive model I engineered several distance based features which contained a lot of predictive power.

Reporting - I reported the results demonstrating the significance of the results with both Classical and Bayesian inference. 
For Classical - I demonstrated that a null hypothesis (both mailing lists having similar performance) is significantly ruled out (low p values; two tail test).
For Bayesian - assuming a Beta distribution, I demonstrated that the Cumulative Distribution Function of the ratio of samples (my distribution over the vendor’s) is significantly over 1. 




 

